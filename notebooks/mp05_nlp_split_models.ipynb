{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or command prompt:\n",
    "#python3 -m spacy download en\n",
    "\n",
    "# Packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Import stopwords and other word packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # as gensimvis  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, coo_matrix\n",
    "\n",
    "\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv('../data/airbnb_gentrification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments_concatenated</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>description</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>property_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>amenities</th>\n",
       "      <th>minimum_nights_avg_ntm</th>\n",
       "      <th>maximum_nights_avg_ntm</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>house_price_2021-01-31</th>\n",
       "      <th>house_pct_change</th>\n",
       "      <th>rentals_2021-01-31</th>\n",
       "      <th>rental_price_pct_change</th>\n",
       "      <th>new_restaurants</th>\n",
       "      <th>available_beer</th>\n",
       "      <th>str_permits_2020</th>\n",
       "      <th>str_permits_growth</th>\n",
       "      <th>crimes</th>\n",
       "      <th>total_pop_2010</th>\n",
       "      <th>total_pop_2019</th>\n",
       "      <th>total_pop_change</th>\n",
       "      <th>total_pop_pct_change</th>\n",
       "      <th>pop_over25_2010</th>\n",
       "      <th>pop_over25_2019</th>\n",
       "      <th>pop_over25_change</th>\n",
       "      <th>pop_over25_pcg_change</th>\n",
       "      <th>total_households_2010</th>\n",
       "      <th>total_households_2019</th>\n",
       "      <th>total_households_change</th>\n",
       "      <th>total_households_pct_change</th>\n",
       "      <th>white_pct_2010</th>\n",
       "      <th>white_pct_2019</th>\n",
       "      <th>white_value_change</th>\n",
       "      <th>white_pct_change</th>\n",
       "      <th>bach_pct_2010</th>\n",
       "      <th>bach_pct_2019</th>\n",
       "      <th>bach_value_change</th>\n",
       "      <th>bach_pct_change</th>\n",
       "      <th>rent_pct_2010</th>\n",
       "      <th>rent_pct_2019</th>\n",
       "      <th>rent_value_change</th>\n",
       "      <th>renter_pct_change</th>\n",
       "      <th>median_hhi_2010</th>\n",
       "      <th>median_hhi_2019</th>\n",
       "      <th>median_hhi_value_change</th>\n",
       "      <th>median_hhi_pct_change</th>\n",
       "      <th>poverty_pct_2010</th>\n",
       "      <th>poverty_pct_2019</th>\n",
       "      <th>poverty_value_change</th>\n",
       "      <th>poverty_pct_change</th>\n",
       "      <th>gentrifying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6422</td>\n",
       "      <td>I can't say enough about how wonderful it was ...</td>\n",
       "      <td>Nashville Charm</td>\n",
       "      <td>12172</td>\n",
       "      <td>36.17315</td>\n",
       "      <td>-86.73581</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>674</td>\n",
       "      <td>4.69</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>https://www.airbnb.com/rooms/6422</td>\n",
       "      <td>30 day or more rental during COVID. Show COVID...</td>\n",
       "      <td>Historic East Nashville is home to many new an...</td>\n",
       "      <td>2009-04-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private room in house</td>\n",
       "      <td>2</td>\n",
       "      <td>1 private bath</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[\"Hair dryer\", \"Bathtub\", \"Lock on bedroom doo...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37206.0</td>\n",
       "      <td>4.703701e+10</td>\n",
       "      <td>412476.0</td>\n",
       "      <td>38.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>-444.0</td>\n",
       "      <td>-0.174528</td>\n",
       "      <td>1703.0</td>\n",
       "      <td>1639.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-0.037581</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>-214.0</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>0.657626</td>\n",
       "      <td>0.940952</td>\n",
       "      <td>0.283327</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.585723</td>\n",
       "      <td>0.177032</td>\n",
       "      <td>0.43317</td>\n",
       "      <td>0.320175</td>\n",
       "      <td>0.240821</td>\n",
       "      <td>-0.079355</td>\n",
       "      <td>-0.247848</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>91643.0</td>\n",
       "      <td>45643.0</td>\n",
       "      <td>0.992239</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.037736</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id                              comments_concatenated             name  host_id  latitude  longitude  price  minimum_nights  number_of_reviews  reviews_per_month  calculated_host_listings_count  availability_365                        listing_url                                        description                              neighborhood_overview  host_since  host_listings_count          property_type  accommodates  bathrooms_text  bedrooms  beds                                          amenities  minimum_nights_avg_ntm  maximum_nights_avg_ntm  review_scores_rating  review_scores_accuracy  review_scores_cleanliness  review_scores_checkin  review_scores_communication  review_scores_location  review_scores_value  zip_code         GEOID  house_price_2021-01-31  house_pct_change  rentals_2021-01-31  rental_price_pct_change  new_restaurants  available_beer  str_permits_2020  str_permits_growth  crimes  total_pop_2010  total_pop_2019  total_pop_change  total_pop_pct_change  \\\n",
       "0        6422  I can't say enough about how wonderful it was ...  Nashville Charm    12172  36.17315  -86.73581     40              30                674               4.69                               1               267  https://www.airbnb.com/rooms/6422  30 day or more rental during COVID. Show COVID...  Historic East Nashville is home to many new an...  2009-04-03                  0.0  Private room in house             2  1 private bath       2.0   3.0  [\"Hair dryer\", \"Bathtub\", \"Lock on bedroom doo...                    30.0                   365.0                  99.0                    10.0                       10.0                   10.0                         10.0                    10.0                 10.0   37206.0  4.703701e+10                412476.0             38.31                 NaN                      NaN              1.0             2.0             114.0               114.0  1165.0          2544.0          2100.0            -444.0             -0.174528   \n",
       "\n",
       "   pop_over25_2010  pop_over25_2019  pop_over25_change  pop_over25_pcg_change  total_households_2010  total_households_2019  total_households_change  total_households_pct_change  white_pct_2010  white_pct_2019  white_value_change  white_pct_change  bach_pct_2010  bach_pct_2019  bach_value_change  bach_pct_change  rent_pct_2010  rent_pct_2019  rent_value_change  renter_pct_change  median_hhi_2010  median_hhi_2019  median_hhi_value_change  median_hhi_pct_change  poverty_pct_2010  poverty_pct_2019  poverty_value_change  poverty_pct_change  gentrifying  \n",
       "0           1703.0           1639.0              -64.0              -0.037581                 1140.0                  926.0                   -214.0                    -0.187719        0.657626        0.940952            0.283327          0.430833       0.408691       0.585723           0.177032          0.43317       0.320175       0.240821          -0.079355          -0.247848          46000.0          91643.0                  45643.0               0.992239              10.6              10.2                  -0.4           -0.037736        False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5205, 76)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = airbnb[airbnb['comments_concatenated'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_nlp = airbnb[['listing_id', 'comments_concatenated', 'gentrifying']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_nlp.comments_concatenated.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into two datasets - gentrifying and non-gentrifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentrifying = airbnb_nlp[airbnb_nlp['gentrifying']==True]\n",
    "non_gentrifying = airbnb_nlp[airbnb_nlp['gentrifying']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists with training and testing apns\n",
    "train_listings_gent, test_listings_gen = tts(gentrifying['listing_id'].to_list(), \n",
    "                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes from the lists of apns\n",
    "train_gentrifying = airbnb_nlp[airbnb_nlp['listing_id'].isin(train_listings_gent)].sort_values('listing_id')\n",
    "test_gentrifying = airbnb_nlp[airbnb_nlp['listing_id'].isin(test_listings_gen)].sort_values('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists with training and testing apns\n",
    "train_listings_non, test_listings_non = tts(non_gentrifying['listing_id'].to_list(), \n",
    "                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes from the lists of apns\n",
    "train_non_gentrifying = airbnb_nlp[airbnb_nlp['listing_id'].isin(train_listings_non)].sort_values('listing_id')\n",
    "test_non_gentrifying = airbnb_nlp[airbnb_nlp['listing_id'].isin(test_listings_non)].sort_values('listing_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA - Gentrifying\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = train_gentrifying.comments_concatenated.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review break symbols\n",
    "data = [re.sub(\"\\\\r\\\\n\", \"\", comment) for comment in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence to words, removing uneeded words/characters\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tagged_words = nltk.tag.pos_tag(sentence.split()) \n",
    "        no_names = [word for word,tag in tagged_words if tag != 'NNP' and tag != 'NNPS'] # Remove proper nouns\n",
    "        yield(gensim.utils.simple_preprocess(str(no_names), deacc=True)) #Clean and remove punctuation\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words,\n",
    "                               min_count=5,\n",
    "                               threshold=100)#,  # higher threshold fewer phrases.\n",
    "                               #connector_words=phrases.ENGLISH_CONNECTOR_WORDS) ***I think I need to download this.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)#, connector_words=phrases.ENGLISH_CONNECTOR_WORDS)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Test trigram on first review\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'stay', 'place', 'location', 'home', 'house', 'host', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eaterie'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to view a single word within the corpus\n",
    "id2word[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('aanvoelen', 1),\n",
       "  ('aardige', 1),\n",
       "  ('abd', 1),\n",
       "  ('ability', 1),\n",
       "  ('able', 20),\n",
       "  ('abode', 1),\n",
       "  ('absence', 1),\n",
       "  ('absolute', 3),\n",
       "  ('absolutely', 26),\n",
       "  ('absorb', 1),\n",
       "  ('abundance', 1),\n",
       "  ('accept', 2),\n",
       "  ('access', 7),\n",
       "  ('accessible', 1),\n",
       "  ('accomadate', 1),\n",
       "  ('accommadition', 1),\n",
       "  ('accommodate', 34),\n",
       "  ('accommodation', 12),\n",
       "  ('accomodation', 1),\n",
       "  ('accompany', 1),\n",
       "  ('accomplish', 1),\n",
       "  ('account', 1),\n",
       "  ('accueil', 1),\n",
       "  ('accurately', 1),\n",
       "  ('across', 1),\n",
       "  ('action', 2),\n",
       "  ('active', 1),\n",
       "  ('actively', 1),\n",
       "  ('activist', 1),\n",
       "  ('activity', 2),\n",
       "  ('actual', 1),\n",
       "  ('actually', 3),\n",
       "  ('add', 2),\n",
       "  ('addition', 2),\n",
       "  ('additional', 1),\n",
       "  ('address', 1),\n",
       "  ('adequately', 1),\n",
       "  ('adjustable', 1),\n",
       "  ('adjustment', 1),\n",
       "  ('adore', 2),\n",
       "  ('adresse', 1),\n",
       "  ('advance', 1),\n",
       "  ('advantage', 1),\n",
       "  ('adventure', 1),\n",
       "  ('advertise', 2),\n",
       "  ('advice', 16),\n",
       "  ('advise', 3),\n",
       "  ('aesthetically_please', 1),\n",
       "  ('affordable', 1),\n",
       "  ('afternoon', 1),\n",
       "  ('age', 1),\n",
       "  ('agin', 1),\n",
       "  ('agio', 1),\n",
       "  ('agreable', 1),\n",
       "  ('agree', 1),\n",
       "  ('air', 6),\n",
       "  ('air_conditione', 1),\n",
       "  ('airbnb', 13),\n",
       "  ('airbnd', 1),\n",
       "  ('airconditione', 1),\n",
       "  ('airport', 6),\n",
       "  ('airy', 3),\n",
       "  ('album', 1),\n",
       "  ('aldaar', 1),\n",
       "  ('allerlei', 1),\n",
       "  ('allow', 2),\n",
       "  ('almost', 3),\n",
       "  ('almost_instantly', 1),\n",
       "  ('alone', 2),\n",
       "  ('along', 1),\n",
       "  ('alovely', 1),\n",
       "  ('already', 2),\n",
       "  ('also', 78),\n",
       "  ('alsolutely', 1),\n",
       "  ('alternative', 2),\n",
       "  ('always', 14),\n",
       "  ('amazing', 43),\n",
       "  ('amble', 1),\n",
       "  ('amenity', 5),\n",
       "  ('american', 3),\n",
       "  ('amount', 2),\n",
       "  ('ample', 2),\n",
       "  ('angel', 1),\n",
       "  ('anime', 1),\n",
       "  ('annual', 1),\n",
       "  ('answer', 4),\n",
       "  ('anthology', 1),\n",
       "  ('antique', 1),\n",
       "  ('anxiety', 1),\n",
       "  ('anymore', 1),\n",
       "  ('anytime', 2),\n",
       "  ('anywhere', 6),\n",
       "  ('apart', 1),\n",
       "  ('apparent', 1),\n",
       "  ('appeal', 1),\n",
       "  ('appear', 1),\n",
       "  ('appoint', 1),\n",
       "  ('appreciate', 9),\n",
       "  ('appreciative', 1),\n",
       "  ('architectural', 1),\n",
       "  ('architecture', 1),\n",
       "  ('area', 86),\n",
       "  ('areal', 1),\n",
       "  ('arguably', 1),\n",
       "  ('around', 12),\n",
       "  ('arrival', 9),\n",
       "  ('arrive', 11),\n",
       "  ('arrivee', 1),\n",
       "  ('art', 4),\n",
       "  ('artist', 4),\n",
       "  ('artsy', 1),\n",
       "  ('artwork', 1),\n",
       "  ('ask', 17),\n",
       "  ('asleep', 1),\n",
       "  ('aspect', 1),\n",
       "  ('assistance', 1),\n",
       "  ('atmosphere', 2),\n",
       "  ('attach', 3),\n",
       "  ('attached', 1),\n",
       "  ('attachment', 1),\n",
       "  ('attend', 4),\n",
       "  ('attendant', 1),\n",
       "  ('attention', 1),\n",
       "  ('attentive', 2),\n",
       "  ('attraction', 6),\n",
       "  ('attractive', 2),\n",
       "  ('auch', 1),\n",
       "  ('aunt', 1),\n",
       "  ('authentic', 8),\n",
       "  ('autonomy', 1),\n",
       "  ('autumn', 1),\n",
       "  ('available', 13),\n",
       "  ('avrete', 1),\n",
       "  ('aware', 1),\n",
       "  ('away', 28),\n",
       "  ('awesome', 11),\n",
       "  ('awkward', 2),\n",
       "  ('back', 50),\n",
       "  ('background', 1),\n",
       "  ('backpack', 1),\n",
       "  ('backyard', 5),\n",
       "  ('bag', 2),\n",
       "  ('bagel', 19),\n",
       "  ('bakery', 13),\n",
       "  ('balance', 2),\n",
       "  ('balcony', 24),\n",
       "  ('balcony_overlooke', 1),\n",
       "  ('balkony', 1),\n",
       "  ('banana', 1),\n",
       "  ('band', 17),\n",
       "  ('bar', 55),\n",
       "  ('barely', 3),\n",
       "  ('base', 5),\n",
       "  ('bat', 1),\n",
       "  ('bath', 40),\n",
       "  ('bath_salt', 3),\n",
       "  ('bathe', 2),\n",
       "  ('bathroom', 60),\n",
       "  ('bathtub', 15),\n",
       "  ('bear', 1),\n",
       "  ('beat', 4),\n",
       "  ('beautiful', 121),\n",
       "  ('beautifull', 1),\n",
       "  ('beautifully', 3),\n",
       "  ('beauty', 2),\n",
       "  ('become', 1),\n",
       "  ('bed', 39),\n",
       "  ('bedroom', 20),\n",
       "  ('beforehand', 1),\n",
       "  ('begin', 2),\n",
       "  ('behalf', 1),\n",
       "  ('behind', 2),\n",
       "  ('bekaman', 1),\n",
       "  ('believe', 1),\n",
       "  ('beliggende', 1),\n",
       "  ('bella', 1),\n",
       "  ('belonging', 1),\n",
       "  ('besseren', 1),\n",
       "  ('beware', 1),\n",
       "  ('bicycle', 1),\n",
       "  ('bien', 1),\n",
       "  ('big', 15),\n",
       "  ('bike', 4),\n",
       "  ('bill', 2),\n",
       "  ('binder', 2),\n",
       "  ('bird', 4),\n",
       "  ('birthday', 2),\n",
       "  ('bit', 13),\n",
       "  ('bite', 1),\n",
       "  ('biz', 1),\n",
       "  ('blanket', 2),\n",
       "  ('blast', 1),\n",
       "  ('blizzard', 1),\n",
       "  ('block', 8),\n",
       "  ('blow', 2),\n",
       "  ('blue', 2),\n",
       "  ('bluegrass', 8),\n",
       "  ('bnb', 2),\n",
       "  ('board', 2),\n",
       "  ('boast', 1),\n",
       "  ('bon', 1),\n",
       "  ('bonus', 5),\n",
       "  ('book', 22),\n",
       "  ('booking', 1),\n",
       "  ('booklet', 1),\n",
       "  ('boot', 1),\n",
       "  ('boring', 1),\n",
       "  ('bother', 2),\n",
       "  ('bottle', 1),\n",
       "  ('bottom', 1),\n",
       "  ('boutique', 1),\n",
       "  ('box', 2),\n",
       "  ('boy', 1),\n",
       "  ('boyfriend', 6),\n",
       "  ('brainer', 1),\n",
       "  ('brand', 1),\n",
       "  ('break', 2),\n",
       "  ('breakfast', 26),\n",
       "  ('breakfast_sandwich', 2),\n",
       "  ('breathe', 1),\n",
       "  ('breeze', 1),\n",
       "  ('brief', 2),\n",
       "  ('bright', 3),\n",
       "  ('brilliant', 5),\n",
       "  ('brim', 1),\n",
       "  ('bring', 6),\n",
       "  ('brit', 1),\n",
       "  ('broad', 1),\n",
       "  ('brunch', 2),\n",
       "  ('brød', 1),\n",
       "  ('bub', 1),\n",
       "  ('bubble', 2),\n",
       "  ('build', 1),\n",
       "  ('bunch', 2),\n",
       "  ('bungalow', 1),\n",
       "  ('bungo', 1),\n",
       "  ('bus', 36),\n",
       "  ('business', 5),\n",
       "  ('busy', 6),\n",
       "  ('buurt', 1),\n",
       "  ('buy', 6),\n",
       "  ('buying', 1),\n",
       "  ('cab', 6),\n",
       "  ('cafe', 15),\n",
       "  ('cake', 1),\n",
       "  ('call', 8),\n",
       "  ('calm', 1),\n",
       "  ('camouflage', 1),\n",
       "  ('cancel', 1),\n",
       "  ('car', 25),\n",
       "  ('card', 1),\n",
       "  ('care', 3),\n",
       "  ('careful', 3),\n",
       "  ('caring', 1),\n",
       "  ('case', 3),\n",
       "  ('cast', 1),\n",
       "  ('casual', 1),\n",
       "  ('catch', 5),\n",
       "  ('cause', 1),\n",
       "  ('cd', 6),\n",
       "  ('ceiling', 3),\n",
       "  ('celebrate', 1),\n",
       "  ('center', 3),\n",
       "  ('central', 2),\n",
       "  ('centrally_locate', 2),\n",
       "  ('centre', 5),\n",
       "  ('century', 2),\n",
       "  ('cereal', 2),\n",
       "  ('certain', 1),\n",
       "  ('certainly', 6),\n",
       "  ('chambre', 2),\n",
       "  ('chance', 10),\n",
       "  ('chanche', 1),\n",
       "  ('change', 3),\n",
       "  ('changedat', 1),\n",
       "  ('character', 12),\n",
       "  ('charm', 9),\n",
       "  ('charme', 1),\n",
       "  ('charmerende', 2),\n",
       "  ('charming', 24),\n",
       "  ('chat', 18),\n",
       "  ('cheap', 2),\n",
       "  ('cheaper', 1),\n",
       "  ('check', 16),\n",
       "  ('checking', 1),\n",
       "  ('checkout', 1),\n",
       "  ('cheerspat', 1),\n",
       "  ('cheese', 3),\n",
       "  ('cherry', 1),\n",
       "  ('chez', 1),\n",
       "  ('chill', 1),\n",
       "  ('chilly', 3),\n",
       "  ('chirp', 1),\n",
       "  ('chocolate', 1),\n",
       "  ('choice', 3),\n",
       "  ('choose', 4),\n",
       "  ('chore', 1),\n",
       "  ('christmas', 1),\n",
       "  ('city', 50),\n",
       "  ('clap', 1),\n",
       "  ('class', 1),\n",
       "  ('claw', 2),\n",
       "  ('claw_foot', 3),\n",
       "  ('clawfoot_tub', 1),\n",
       "  ('clean', 93),\n",
       "  ('cleanliness', 1),\n",
       "  ('clear', 2),\n",
       "  ('clearly', 5),\n",
       "  ('climb', 1),\n",
       "  ('close', 48),\n",
       "  ('closet', 3),\n",
       "  ('clothe', 3),\n",
       "  ('club', 5),\n",
       "  ('co', 1),\n",
       "  ('cocktail', 2),\n",
       "  ('code', 1),\n",
       "  ('coffee', 71),\n",
       "  ('coffeehouse', 1),\n",
       "  ('coffeeshop', 1),\n",
       "  ('cold', 5),\n",
       "  ('colleague', 1),\n",
       "  ('collect', 1),\n",
       "  ('collection', 6),\n",
       "  ('color', 2),\n",
       "  ('colorful', 1),\n",
       "  ('combination', 1),\n",
       "  ('combine', 1),\n",
       "  ('come', 52),\n",
       "  ('comfort', 2),\n",
       "  ('comfortable', 113),\n",
       "  ('comfortably', 2),\n",
       "  ('comforting', 1),\n",
       "  ('comfy', 11),\n",
       "  ('comme', 1),\n",
       "  ('comment', 1),\n",
       "  ('common', 4),\n",
       "  ('communicate', 2),\n",
       "  ('communication', 3),\n",
       "  ('communicator', 1),\n",
       "  ('community', 7),\n",
       "  ('commute', 1),\n",
       "  ('compact', 1),\n",
       "  ('company', 3),\n",
       "  ('compare', 1),\n",
       "  ('compassionate', 1),\n",
       "  ('complaint', 1),\n",
       "  ('complete', 2),\n",
       "  ('completely', 5),\n",
       "  ('con', 2),\n",
       "  ('concern', 2),\n",
       "  ('concert', 7),\n",
       "  ('condition', 3),\n",
       "  ('conditioner', 2),\n",
       "  ('conf', 1),\n",
       "  ('confirm', 2),\n",
       "  ('confortable', 1),\n",
       "  ('congenial', 1),\n",
       "  ('connaissent', 1),\n",
       "  ('connect', 2),\n",
       "  ('connected', 1),\n",
       "  ('connection', 1),\n",
       "  ('conserve', 1),\n",
       "  ('consider', 2),\n",
       "  ('considerate', 5),\n",
       "  ('consigliero', 1),\n",
       "  ('contact', 1),\n",
       "  ('container', 1),\n",
       "  ('convenable', 1),\n",
       "  ('convenience', 1),\n",
       "  ('convenient', 18),\n",
       "  ('conveniently_locate', 4),\n",
       "  ('conversation', 17),\n",
       "  ('convivial', 1),\n",
       "  ('cooking', 1),\n",
       "  ('cool', 34),\n",
       "  ('copy', 1),\n",
       "  ('corner', 7),\n",
       "  ('cost', 1),\n",
       "  ('cosy', 2),\n",
       "  ('coughing', 1),\n",
       "  ('countless', 1),\n",
       "  ('country', 4),\n",
       "  ('couple', 21),\n",
       "  ('course', 1),\n",
       "  ('courteous', 1),\n",
       "  ('cowboy', 1),\n",
       "  ('cozy', 21),\n",
       "  ('craft', 1),\n",
       "  ('craftsman', 4),\n",
       "  ('craig', 1),\n",
       "  ('cream', 2),\n",
       "  ('creative', 5),\n",
       "  ('crow', 1),\n",
       "  ('crowd', 1),\n",
       "  ('crowded', 1),\n",
       "  ('cuisine', 1),\n",
       "  ('culture', 2),\n",
       "  ('cultuur', 1),\n",
       "  ('cumbersome', 1),\n",
       "  ('cup', 1),\n",
       "  ('cupboard', 1),\n",
       "  ('curious', 1),\n",
       "  ('curl', 1),\n",
       "  ('current', 1),\n",
       "  ('curtain', 5),\n",
       "  ('cut', 1),\n",
       "  ('cute', 7),\n",
       "  ('cycle', 1),\n",
       "  ('daily', 1),\n",
       "  ('damnit', 1),\n",
       "  ('damp', 1),\n",
       "  ('dan', 1),\n",
       "  ('dark', 1),\n",
       "  ('dates', 1),\n",
       "  ('daughter', 5),\n",
       "  ('day', 35),\n",
       "  ('deal', 4),\n",
       "  ('decide', 2),\n",
       "  ('decision', 1),\n",
       "  ('deck', 11),\n",
       "  ('decor', 3),\n",
       "  ('decorate', 4),\n",
       "  ('dedicated', 1),\n",
       "  ('deep', 1),\n",
       "  ('def', 1),\n",
       "  ('defiantly', 1),\n",
       "  ('definite', 1),\n",
       "  ('definitely', 65),\n",
       "  ('delay', 1),\n",
       "  ('delicious', 3),\n",
       "  ('delight', 6),\n",
       "  ('delighted', 3),\n",
       "  ('delightful', 7),\n",
       "  ('demonstrate', 1),\n",
       "  ('departure', 3),\n",
       "  ('depict', 2),\n",
       "  ('depot', 2),\n",
       "  ('dere', 4),\n",
       "  ('describe', 12),\n",
       "  ('description', 4),\n",
       "  ('desire', 1),\n",
       "  ('desk', 1),\n",
       "  ('destination', 2),\n",
       "  ('detail', 7),\n",
       "  ('detailed', 2),\n",
       "  ('die', 2),\n",
       "  ('difference', 1),\n",
       "  ('different', 6),\n",
       "  ('difficult', 8),\n",
       "  ('dine', 2),\n",
       "  ('dining', 2),\n",
       "  ('dinner', 5),\n",
       "  ('direct', 3),\n",
       "  ('direction', 6),\n",
       "  ('directly', 1),\n",
       "  ('disability', 1),\n",
       "  ('disadvantage', 1),\n",
       "  ('disappoint', 2),\n",
       "  ('disappointed', 2),\n",
       "  ('discover', 2),\n",
       "  ('discribe', 1),\n",
       "  ('discuss', 3),\n",
       "  ('discussion', 2),\n",
       "  ('distance', 67),\n",
       "  ('district', 2),\n",
       "  ('do', 5),\n",
       "  ('doable', 1),\n",
       "  ('dog', 1),\n",
       "  ('door', 12),\n",
       "  ('downfall', 1),\n",
       "  ('downside', 1),\n",
       "  ('downstair', 1),\n",
       "  ('downstairs', 2),\n",
       "  ('downtown', 89),\n",
       "  ('dowtown', 1),\n",
       "  ('dr', 2),\n",
       "  ('drawback', 1),\n",
       "  ('dream', 3),\n",
       "  ('drift', 1),\n",
       "  ('drink', 7),\n",
       "  ('drinking', 1),\n",
       "  ('drive', 36),\n",
       "  ('driver', 1),\n",
       "  ('driving', 1),\n",
       "  ('drop', 2),\n",
       "  ('drunk', 1),\n",
       "  ('dryer', 2),\n",
       "  ('due', 3),\n",
       "  ('duty', 1),\n",
       "  ('dvr', 1),\n",
       "  ('e', 1),\n",
       "  ('eager', 4),\n",
       "  ('early', 8),\n",
       "  ('earth', 1),\n",
       "  ('ease', 2),\n",
       "  ('easily', 3),\n",
       "  ('easily_accessible', 2),\n",
       "  ('east', 14),\n",
       "  ('easy', 52),\n",
       "  ('eat', 46),\n",
       "  ('eaterie', 3),\n",
       "  ('eatery', 4),\n",
       "  ('eating', 2),\n",
       "  ('eave', 1),\n",
       "  ('eclectic', 5),\n",
       "  ('educate', 1),\n",
       "  ('een', 3),\n",
       "  ('eerste', 1),\n",
       "  ('egg', 2),\n",
       "  ('ekstra', 1),\n",
       "  ('election', 1),\n",
       "  ('elegant', 1),\n",
       "  ('else', 8),\n",
       "  ('email', 1),\n",
       "  ('emergency', 1),\n",
       "  ('enable', 1),\n",
       "  ('encourage', 3),\n",
       "  ('end', 7),\n",
       "  ('energetic', 1),\n",
       "  ('energy', 2),\n",
       "  ('engage', 4),\n",
       "  ('enhance', 1),\n",
       "  ('enhanced', 2),\n",
       "  ('enjoy', 105),\n",
       "  ('enjoyable', 5),\n",
       "  ('enormous', 1),\n",
       "  ('enough', 22),\n",
       "  ('enrich', 1),\n",
       "  ('ensuite', 3),\n",
       "  ('ensure', 3),\n",
       "  ('enter', 1),\n",
       "  ('entertainment', 4),\n",
       "  ('enthusiastic', 1),\n",
       "  ('entire', 5),\n",
       "  ('entirely', 2),\n",
       "  ('entrance', 1),\n",
       "  ('environment', 2),\n",
       "  ('equipped', 1),\n",
       "  ('erect', 1),\n",
       "  ('erg', 1),\n",
       "  ('erste', 1),\n",
       "  ('ervare', 1),\n",
       "  ('escape', 2),\n",
       "  ('especially', 21),\n",
       "  ('esperienza', 1),\n",
       "  ('essence', 1),\n",
       "  ('essential', 2),\n",
       "  ('essentially', 1),\n",
       "  ('est_tre', 1),\n",
       "  ('et', 7),\n",
       "  ('ete', 1),\n",
       "  ('european', 2),\n",
       "  ('even', 45),\n",
       "  ('evening', 13),\n",
       "  ('event', 8),\n",
       "  ('ever', 12),\n",
       "  ('everthing', 1),\n",
       "  ('everywhere', 4),\n",
       "  ('evidence', 1),\n",
       "  ('exactly', 12),\n",
       "  ('example', 1),\n",
       "  ('exceed', 2),\n",
       "  ('excellent', 36),\n",
       "  ('exceptional', 3),\n",
       "  ('exceptionally', 1),\n",
       "  ('excuse', 1),\n",
       "  ('exemplary', 1),\n",
       "  ('exercise', 1),\n",
       "  ('expect', 4),\n",
       "  ('expectation', 3),\n",
       "  ('expensive', 1),\n",
       "  ('experience', 78),\n",
       "  ('expertise', 1),\n",
       "  ('explain', 5),\n",
       "  ('exploratory', 1),\n",
       "  ('explore', 29),\n",
       "  ('explorer', 1),\n",
       "  ('extension', 1),\n",
       "  ('extensive', 2),\n",
       "  ('extra', 7),\n",
       "  ('extraordinarily', 1),\n",
       "  ('extremely', 22),\n",
       "  ('eye', 1),\n",
       "  ('fab', 1),\n",
       "  ('fabulous', 8),\n",
       "  ('face', 1),\n",
       "  ('facility', 2),\n",
       "  ('fact', 1),\n",
       "  ('fall', 2),\n",
       "  ('familiar', 2),\n",
       "  ('family', 5),\n",
       "  ('famous', 1),\n",
       "  ('fan', 4),\n",
       "  ('fantastic', 40),\n",
       "  ('fantastically', 1),\n",
       "  ('fantastische', 1),\n",
       "  ('far', 10),\n",
       "  ('fare', 1),\n",
       "  ('farewell', 1),\n",
       "  ('farmer_market', 1),\n",
       "  ('fascinating', 3),\n",
       "  ('fashionable', 1),\n",
       "  ('fault', 2),\n",
       "  ('favor', 2),\n",
       "  ('favorite', 7),\n",
       "  ('favourite', 1),\n",
       "  ('feature', 5),\n",
       "  ('feed', 1),\n",
       "  ('feel', 114),\n",
       "  ('felt', 1),\n",
       "  ('feminine', 1),\n",
       "  ('festival', 4),\n",
       "  ('fi', 1),\n",
       "  ('fiddle', 1),\n",
       "  ('fiddler', 1),\n",
       "  ('fight', 1),\n",
       "  ('figure', 1),\n",
       "  ('fijne', 2),\n",
       "  ('fik', 1),\n",
       "  ('fill', 10),\n",
       "  ('filming', 1),\n",
       "  ('final', 1),\n",
       "  ('finally', 1),\n",
       "  ('find', 27),\n",
       "  ('fine', 7),\n",
       "  ('finish', 2),\n",
       "  ('fire', 1),\n",
       "  ('firefly', 1),\n",
       "  ('firm', 1),\n",
       "  ('first', 45),\n",
       "  ('fitting', 1),\n",
       "  ('five_point', 8),\n",
       "  ('flair', 1),\n",
       "  ('flavour', 1),\n",
       "  ('flexibility', 1),\n",
       "  ('flexible', 4),\n",
       "  ('flight', 6),\n",
       "  ('flood', 1),\n",
       "  ('floor', 10),\n",
       "  ('fluff', 2),\n",
       "  ('folk', 5),\n",
       "  ('follow', 1),\n",
       "  ('font', 1),\n",
       "  ('food', 27),\n",
       "  ('foot', 4),\n",
       "  ('forever', 1),\n",
       "  ('forget', 2),\n",
       "  ('form', 1),\n",
       "  ('former', 1),\n",
       "  ('forslag', 1),\n",
       "  ('forth', 1),\n",
       "  ('fortunate', 2),\n",
       "  ('forward', 4),\n",
       "  ('fotos', 1),\n",
       "  ('fracture', 1),\n",
       "  ('free', 2),\n",
       "  ('freedom', 1),\n",
       "  ('freeway', 1),\n",
       "  ('freight', 1),\n",
       "  ('frequently', 2),\n",
       "  ('fresh', 2),\n",
       "  ('fridge', 3),\n",
       "  ('friend', 16),\n",
       "  ('friendly', 53),\n",
       "  ('frill', 1),\n",
       "  ('frizzante', 1),\n",
       "  ('front', 3),\n",
       "  ('fruit', 2),\n",
       "  ('full', 25),\n",
       "  ('fun', 28),\n",
       "  ('funky', 4),\n",
       "  ('funnest', 1),\n",
       "  ('furnishing', 1),\n",
       "  ('fuss', 1),\n",
       "  ('future', 8),\n",
       "  ('future_visit', 1),\n",
       "  ('g', 1),\n",
       "  ('gal', 1),\n",
       "  ('gallery', 2),\n",
       "  ('garden', 3),\n",
       "  ('gastvrij', 1),\n",
       "  ('gav', 1),\n",
       "  ('gekregen', 1),\n",
       "  ('gelijk', 1),\n",
       "  ('gem', 3),\n",
       "  ('general', 4),\n",
       "  ('generally', 1),\n",
       "  ('generosity', 3),\n",
       "  ('generous', 8),\n",
       "  ('generously', 1),\n",
       "  ('genre', 2),\n",
       "  ('gentle', 2),\n",
       "  ('gently', 1),\n",
       "  ('genuine', 6),\n",
       "  ('genuinely', 2),\n",
       "  ('gesprek', 1),\n",
       "  ('get', 107),\n",
       "  ('getaway', 1),\n",
       "  ('gevindst', 1),\n",
       "  ('gewenst', 1),\n",
       "  ('gift', 1),\n",
       "  ('gig', 4),\n",
       "  ('gigs', 1),\n",
       "  ('girlfriend', 9),\n",
       "  ('give', 74),\n",
       "  ('giving', 1),\n",
       "  ('glad', 5),\n",
       "  ('gladly', 2),\n",
       "  ('glimpse', 1),\n",
       "  ('go', 105),\n",
       "  ('gode', 1),\n",
       "  ('good', 105),\n",
       "  ('gorgeous', 14),\n",
       "  ('gourmet', 1),\n",
       "  ('grab', 3),\n",
       "  ('grace', 1),\n",
       "  ('gracious', 20),\n",
       "  ('graciously', 1),\n",
       "  ('gradually', 1),\n",
       "  ('graduation', 1),\n",
       "  ('grant', 1),\n",
       "  ('grass', 1),\n",
       "  ('grateful', 4),\n",
       "  ('great', 1),\n",
       "  ('greatly', 1),\n",
       "  ('greatly_appreciate', 1),\n",
       "  ('greatly_appreciated', 1),\n",
       "  ('green', 2),\n",
       "  ('greenery', 1),\n",
       "  ('greenway', 2),\n",
       "  ('greet', 2),\n",
       "  ('grocery_store', 1),\n",
       "  ('groovey', 1),\n",
       "  ('groovy', 1),\n",
       "  ('guess', 1),\n",
       "  ('guest', 10),\n",
       "  ('guesthouse', 1),\n",
       "  ('guestroom', 1),\n",
       "  ('guidance', 1),\n",
       "  ('guide', 4),\n",
       "  ('guitar', 5),\n",
       "  ('guru', 1),\n",
       "  ('gute', 1),\n",
       "  ('guy', 8),\n",
       "  ('habit', 1),\n",
       "  ('hair', 3),\n",
       "  ('hair_dryer', 1),\n",
       "  ('half', 3),\n",
       "  ('hand', 11),\n",
       "  ('handful', 1),\n",
       "  ('handheld', 2),\n",
       "  ('handy', 3),\n",
       "  ('hang', 1),\n",
       "  ('hangout', 1),\n",
       "  ('happen', 3),\n",
       "  ('happening', 1),\n",
       "  ('happy', 13),\n",
       "  ('har', 1),\n",
       "  ('hard', 4),\n",
       "  ('hardly', 2),\n",
       "  ('harmless', 1),\n",
       "  ('hatten', 1),\n",
       "  ('head', 3),\n",
       "  ('health', 2),\n",
       "  ('healthy', 1),\n",
       "  ('hear', 21),\n",
       "  ('hearing', 9),\n",
       "  ('heart', 9),\n",
       "  ('heartbeat', 1),\n",
       "  ('heartfelt', 1),\n",
       "  ('heat', 2),\n",
       "  ('heated', 2),\n",
       "  ('heater', 4),\n",
       "  ('heating', 1),\n",
       "  ('heavy', 1),\n",
       "  ('hebben', 1),\n",
       "  ('hele', 2),\n",
       "  ('help', 24),\n",
       "  ('helpful', 62),\n",
       "  ('helpfull', 1),\n",
       "  ('heritage', 2),\n",
       "  ('hesitate', 3),\n",
       "  ('hesitation', 2),\n",
       "  ('het', 2),\n",
       "  ('hide', 2),\n",
       "  ('highlight', 4),\n",
       "  ('highly', 46),\n",
       "  ('hiking', 1),\n",
       "  ('hint', 1),\n",
       "  ('hip', 7),\n",
       "  ('hipst', 1),\n",
       "  ('historian', 1),\n",
       "  ('historic', 19),\n",
       "  ('historical', 3),\n",
       "  ('history', 8),\n",
       "  ('hit', 1),\n",
       "  ('hoat', 1),\n",
       "  ('hobby', 1),\n",
       "  ('hold', 7),\n",
       "  ('holiday', 1),\n",
       "  ('hollow', 1),\n",
       "  ('home', 3),\n",
       "  ('homely', 1),\n",
       "  ('homey', 3),\n",
       "  ('honestly', 1),\n",
       "  ('honeymoon', 1),\n",
       "  ('honkeytonk', 1),\n",
       "  ('honky_tonkin', 1),\n",
       "  ('honkytonk', 1),\n",
       "  ('hooked', 1),\n",
       "  ('hoopla', 1),\n",
       "  ('hop', 1),\n",
       "  ('hope', 25),\n",
       "  ('hopefully', 1),\n",
       "  ('hose', 2),\n",
       "  ('hospitable', 6),\n",
       "  ('hospitality', 11),\n",
       "  ('host', 254),\n",
       "  ('hot', 8),\n",
       "  ('hote', 2),\n",
       "  ('hotel', 11),\n",
       "  ('hotspot', 3),\n",
       "  ('hour', 10),\n",
       "  ('house', 9),\n",
       "  ('household', 1),\n",
       "  ('however', 3),\n",
       "  ('hub', 1),\n",
       "  ('hubby', 1),\n",
       "  ('hug', 1),\n",
       "  ('huge', 7),\n",
       "  ('huis', 1),\n",
       "  ('humor', 1),\n",
       "  ('hunt', 1),\n",
       "  ('hunting', 1),\n",
       "  ('hurt', 1),\n",
       "  ('hus', 1),\n",
       "  ('husband', 34),\n",
       "  ('hvad', 1),\n",
       "  ('ice', 2),\n",
       "  ('ice_storm', 1),\n",
       "  ('idea', 8),\n",
       "  ('ideal', 7),\n",
       "  ('ideally', 1),\n",
       "  ('il', 1),\n",
       "  ('imaginably', 1),\n",
       "  ('imagine', 2),\n",
       "  ('immaculate', 4),\n",
       "  ('immaculately', 1),\n",
       "  ('immediately', 4),\n",
       "  ('impact', 1),\n",
       "  ('important', 1),\n",
       "  ('importantly', 1),\n",
       "  ('impose', 1),\n",
       "  ('impressive', 2),\n",
       "  ('imødekommende', 1),\n",
       "  ('in', 1),\n",
       "  ('incide', 1),\n",
       "  ('include', 8),\n",
       "  ('inconvenience', 1),\n",
       "  ('inconvenient', 1),\n",
       "  ('incredible', 4),\n",
       "  ('incredibly', 10),\n",
       "  ('indeed', 2),\n",
       "  ('indipendent', 1),\n",
       "  ('industry', 1),\n",
       "  ('inexpensive', 2),\n",
       "  ('info', 10),\n",
       "  ('inform', 1),\n",
       "  ('informal', 1),\n",
       "  ('information', 14),\n",
       "  ('informative', 9),\n",
       "  ('initial', 1),\n",
       "  ('insider', 5),\n",
       "  ('insight', 2),\n",
       "  ('instant', 1),\n",
       "  ('instantly', 1),\n",
       "  ('instead', 1),\n",
       "  ('instruction', 1),\n",
       "  ('instrument', 1),\n",
       "  ('insure', 1),\n",
       "  ('intelligent', 1),\n",
       "  ('intend', 2),\n",
       "  ('interact', 2),\n",
       "  ('interaction', 1),\n",
       "  ('interactive', 1),\n",
       "  ('interest', 7),\n",
       "  ('interesting', 35),\n",
       "  ('internet', 2),\n",
       "  ('interrupt', 1),\n",
       "  ('interview', 1),\n",
       "  ('intrigue', 1),\n",
       "  ('introduce', 4),\n",
       "  ('introduction', 1),\n",
       "  ('intrude', 1),\n",
       "  ('intrusive', 3),\n",
       "  ('invaluable', 4),\n",
       "  ('invasive', 1),\n",
       "  ('invite', 9),\n",
       "  ('inviting', 3),\n",
       "  ('involve', 2),\n",
       "  ('iron', 3),\n",
       "  ('issue', 5),\n",
       "  ('item', 1),\n",
       "  ('jackpot', 1),\n",
       "  ('jake', 1),\n",
       "  ('jam', 5),\n",
       "  ('jammed', 1),\n",
       "  ('jamming', 1),\n",
       "  ('java', 1),\n",
       "  ('je', 1),\n",
       "  ('jeni', 1),\n",
       "  ('jet', 1),\n",
       "  ('job', 2),\n",
       "  ('journey', 1),\n",
       "  ('joy', 2),\n",
       "  ('juice', 2),\n",
       "  ('junkie', 1),\n",
       "  ('kaffe', 1),\n",
       "  ('kan', 1),\n",
       "  ('keen', 1),\n",
       "  ('keep', 11),\n",
       "  ('keeper', 1),\n",
       "  ('keine', 1),\n",
       "  ('key', 1),\n",
       "  ('keyed', 1),\n",
       "  ('killer', 1),\n",
       "  ('kind', 52),\n",
       "  ('kindly', 1),\n",
       "  ('kindness', 1),\n",
       "  ('kitchen', 12),\n",
       "  ('km', 1),\n",
       "  ('knee', 1),\n",
       "  ('knoledge', 1),\n",
       "  ('know', 27),\n",
       "  ('knowledgable', 3),\n",
       "  ('knowledge', 19),\n",
       "  ('knowledgeable', 15),\n",
       "  ('konnen', 2),\n",
       "  ('køkken', 1),\n",
       "  ('la', 1),\n",
       "  ('lack', 2),\n",
       "  ('ladybug', 1),\n",
       "  ('lag', 1),\n",
       "  ('laidback', 3),\n",
       "  ('lake', 1),\n",
       "  ('land', 1),\n",
       "  ('lane', 1),\n",
       "  ('large', 11),\n",
       "  ('last', 10),\n",
       "  ('late', 8),\n",
       "  ('later', 2),\n",
       "  ('launch', 1),\n",
       "  ('laundry_facilitie', 1),\n",
       "  ('lay', 4),\n",
       "  ('lead', 2),\n",
       "  ('leaflet', 1),\n",
       "  ('leafy', 5),\n",
       "  ('learn', 4),\n",
       "  ('learning', 1),\n",
       "  ('least', 1),\n",
       "  ('leave', 23),\n",
       "  ('lend', 1),\n",
       "  ('less', 5),\n",
       "  ('let', 16),\n",
       "  ('level', 1),\n",
       "  ('library', 3),\n",
       "  ('lief', 1),\n",
       "  ('life', 10),\n",
       "  ('lifestyle', 1),\n",
       "  ('lift', 2),\n",
       "  ('lige', 1),\n",
       "  ('light', 7),\n",
       "  ('like', 5),\n",
       "  ('limitation', 1),\n",
       "  ('line', 2),\n",
       "  ('linger', 1),\n",
       "  ('list', 9),\n",
       "  ('listen', 16),\n",
       "  ('listening', 4),\n",
       "  ('listing', 2),\n",
       "  ('little', 27),\n",
       "  ('live', 32),\n",
       "  ('lively', 3),\n",
       "  ('living', 2),\n",
       "  ('load', 6),\n",
       "  ('local', 71),\n",
       "  ('locale', 1),\n",
       "  ('locally', 6),\n",
       "  ('locate', 22),\n",
       "  ('location', 3),\n",
       "  ('lock', 5),\n",
       "  ('locking', 2),\n",
       "  ('loft', 1),\n",
       "  ('log', 1),\n",
       "  ('long', 15),\n",
       "  ('look', 36),\n",
       "  ('look_forward', 4),\n",
       "  ('lose', 1),\n",
       "  ('lost', 2),\n",
       "  ('lot', 69),\n",
       "  ...]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,        # Replace with gensim.models.ldamodel.LdaModel()\n",
    "                       id2word=id2word,\n",
    "                       num_topics=8, #number of topics to identify\n",
    "                       random_state=100,\n",
    "                       #update_every=1,                          #Add back in with LdaModel\n",
    "                       chunksize=100, #number of documents to pass per chunk\n",
    "                       passes=10, #number of training passes\n",
    "                       #alpha='auto',                            #Add back in with LdaModel\n",
    "                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"view\" + 0.016*\"rooftop\" + 0.015*\"clean\" + 0.014*\"recommend\" + '\n",
      "  '0.014*\"downtown\" + 0.013*\"space\" + 0.011*\"nice\" + 0.011*\"group\" + '\n",
      "  '0.010*\"beautiful\" + 0.010*\"amazing\"'),\n",
      " (1,\n",
      "  '0.019*\"clean\" + 0.016*\"group\" + 0.015*\"recommend\" + 0.014*\"perfect\" + '\n",
      "  '0.014*\"downtown\" + 0.013*\"definitely\" + 0.012*\"space\" + 0.011*\"need\" + '\n",
      "  '0.010*\"time\" + 0.010*\"comfortable\"'),\n",
      " (2,\n",
      "  '0.160*\"cottage\" + 0.013*\"cozy\" + 0.011*\"tiny\" + 0.010*\"cute\" + 0.010*\"need\" '\n",
      "  '+ 0.009*\"little\" + 0.007*\"chicken\" + 0.006*\"perfect\" + 0.006*\"recommend\" + '\n",
      "  '0.006*\"clean\"'),\n",
      " (3,\n",
      "  '0.021*\"clean\" + 0.019*\"walk\" + 0.017*\"downtown\" + 0.013*\"comfortable\" + '\n",
      "  '0.013*\"nice\" + 0.012*\"room\" + 0.012*\"recommend\" + 0.011*\"easy\" + '\n",
      "  '0.010*\"definitely\" + 0.010*\"apartment\"'),\n",
      " (4,\n",
      "  '0.016*\"clean\" + 0.014*\"comfortable\" + 0.014*\"recommend\" + 0.012*\"space\" + '\n",
      "  '0.012*\"need\" + 0.011*\"perfect\" + 0.011*\"definitely\" + 0.011*\"stay\" + '\n",
      "  '0.011*\"love\" + 0.010*\"well\"'),\n",
      " (5,\n",
      "  '0.035*\"dog\" + 0.031*\"clean\" + 0.023*\"space\" + 0.017*\"recommend\" + '\n",
      "  '0.014*\"cute\" + 0.013*\"comfortable\" + 0.012*\"love\" + 0.012*\"yard\" + '\n",
      "  '0.012*\"definitely\" + 0.010*\"nice\"'),\n",
      " (6,\n",
      "  '0.024*\"clean\" + 0.023*\"downtown\" + 0.016*\"close\" + 0.012*\"nice\" + '\n",
      "  '0.012*\"need\" + 0.011*\"definitely\" + 0.011*\"comfortable\" + 0.011*\"recommend\" '\n",
      "  '+ 0.010*\"perfect\" + 0.009*\"quick\"'),\n",
      " (7,\n",
      "  '0.024*\"clean\" + 0.015*\"recommend\" + 0.015*\"amazing\" + 0.013*\"definitely\" + '\n",
      "  '0.011*\"make\" + 0.011*\"stay\" + 0.011*\"space\" + 0.010*\"stylish\" + '\n",
      "  '0.009*\"nice\" + 0.008*\"condo\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 Keywords in each grouped Topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.546841952897288\n",
      "\n",
      "Coherence Score:  0.27604132114600477\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score - Likely more helpful. Takes a while to run.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual for Viewing each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda_gentrifying.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipping step 17 about finding the best number of topics - Tim recommends 8-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant Topic in each Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-57075f752869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "\n",
    "for pdf, doc in tqdm(zip(pdfs, docs)):\n",
    "    bow = corpora.Dictionary(data_lemmatized)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    for res in topics:\n",
    "        topic, pct = res\n",
    "        contents.append({'pdf': pdf, 'topic': topic, 'percent': pct})\n",
    "\n",
    "topics = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get main topic in each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_topic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most representative Reviews for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "\n",
    "        file = topics[topics.topic == i].sort_values('percent', ascending = False).head().iloc[j, 0]\n",
    "\n",
    "        cmd = f'cp {file} representative_docs/topic_{i + 1}/.'\n",
    "\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA - Non-Gentrifying\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = train_non_gentrifying.comments_concatenated.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review break symbols\n",
    "data = [re.sub(\"\\\\r\\\\n\", \"\", comment) for comment in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence to words, removing uneeded words/characters\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tagged_words = nltk.tag.pos_tag(sentence.split()) \n",
    "        no_names = [word for word,tag in tagged_words if tag != 'NNP' and tag != 'NNPS'] # Remove proper nouns\n",
    "        yield(gensim.utils.simple_preprocess(str(no_names), deacc=True)) #Clean and remove punctuation\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words,\n",
    "                               min_count=5,\n",
    "                               threshold=100)#,  # higher threshold fewer phrases.\n",
    "                               #connector_words=phrases.ENGLISH_CONNECTOR_WORDS) ***I think I need to download this.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)#, connector_words=phrases.ENGLISH_CONNECTOR_WORDS)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Test trigram on first review\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'stay', 'place', 'location', 'home', 'house', 'host', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trail'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to view a single word within the corpus\n",
    "id2word[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('able', 3),\n",
       "  ('absolutely', 2),\n",
       "  ('access', 3),\n",
       "  ('accessible', 1),\n",
       "  ('accommodate', 3),\n",
       "  ('accommodation', 2),\n",
       "  ('acre', 1),\n",
       "  ('action', 1),\n",
       "  ('activity', 1),\n",
       "  ('actor', 1),\n",
       "  ('actual', 1),\n",
       "  ('actually', 1),\n",
       "  ('add', 1),\n",
       "  ('advertised', 1),\n",
       "  ('affordable', 1),\n",
       "  ('air', 2),\n",
       "  ('airbnb', 5),\n",
       "  ('airport', 2),\n",
       "  ('allow', 1),\n",
       "  ('alone', 2),\n",
       "  ('already', 1),\n",
       "  ('also', 12),\n",
       "  ('always', 3),\n",
       "  ('amazing', 5),\n",
       "  ('amenity', 5),\n",
       "  ('ample', 1),\n",
       "  ('answer', 2),\n",
       "  ('anymore', 1),\n",
       "  ('apartment', 57),\n",
       "  ('appearance', 1),\n",
       "  ('appliance', 1),\n",
       "  ('appoint', 1),\n",
       "  ('appreciator', 1),\n",
       "  ('apt', 4),\n",
       "  ('area', 9),\n",
       "  ('arrival', 4),\n",
       "  ('arrive', 5),\n",
       "  ('art', 4),\n",
       "  ('artwork', 1),\n",
       "  ('ask', 5),\n",
       "  ('aspect', 1),\n",
       "  ('attend', 1),\n",
       "  ('available', 5),\n",
       "  ('aware', 1),\n",
       "  ('away', 5),\n",
       "  ('awesome', 2),\n",
       "  ('back', 10),\n",
       "  ('background', 1),\n",
       "  ('backyard', 11),\n",
       "  ('bad', 1),\n",
       "  ('bake', 1),\n",
       "  ('baked_cookie', 1),\n",
       "  ('ball', 1),\n",
       "  ('base', 1),\n",
       "  ('basement', 1),\n",
       "  ('beat', 2),\n",
       "  ('beautiful', 8),\n",
       "  ('beautifully', 1),\n",
       "  ('beauty', 2),\n",
       "  ('bed', 3),\n",
       "  ('believe', 2),\n",
       "  ('big', 1),\n",
       "  ('bike', 1),\n",
       "  ('biking_trail', 1),\n",
       "  ('bird', 1),\n",
       "  ('bit', 2),\n",
       "  ('block', 2),\n",
       "  ('blue', 1),\n",
       "  ('boat', 1),\n",
       "  ('book', 2),\n",
       "  ('bore', 1),\n",
       "  ('borrow', 1),\n",
       "  ('bother', 1),\n",
       "  ('breathtake', 1),\n",
       "  ('breathtaking', 2),\n",
       "  ('bright', 2),\n",
       "  ('bring', 2),\n",
       "  ('broadway', 1),\n",
       "  ('btw', 1),\n",
       "  ('bug', 1),\n",
       "  ('business', 2),\n",
       "  ('bustle', 1),\n",
       "  ('busy', 1),\n",
       "  ('cabin', 1),\n",
       "  ('cable', 2),\n",
       "  ('call', 2),\n",
       "  ('camera', 1),\n",
       "  ('car', 1),\n",
       "  ('case', 1),\n",
       "  ('casual', 1),\n",
       "  ('central', 2),\n",
       "  ('certain', 1),\n",
       "  ('certainly', 2),\n",
       "  ('chance', 1),\n",
       "  ('chat', 1),\n",
       "  ('check', 2),\n",
       "  ('cheerful', 1),\n",
       "  ('chic', 1),\n",
       "  ('child', 1),\n",
       "  ('choose', 1),\n",
       "  ('city', 5),\n",
       "  ('clean', 11),\n",
       "  ('cleanliness', 1),\n",
       "  ('clever', 1),\n",
       "  ('close', 8),\n",
       "  ('collectible', 1),\n",
       "  ('colorful', 1),\n",
       "  ('come', 7),\n",
       "  ('comfort', 1),\n",
       "  ('comfortable', 17),\n",
       "  ('comfortably', 1),\n",
       "  ('comfy', 1),\n",
       "  ('communicate', 1),\n",
       "  ('communicative', 1),\n",
       "  ('community', 1),\n",
       "  ('commute', 1),\n",
       "  ('complete', 2),\n",
       "  ('completely', 1),\n",
       "  ('con', 1),\n",
       "  ('concert', 1),\n",
       "  ('consider', 1),\n",
       "  ('constant', 1),\n",
       "  ('contact', 1),\n",
       "  ('contain', 1),\n",
       "  ('control', 1),\n",
       "  ('convenience', 2),\n",
       "  ('convenient', 1),\n",
       "  ('cook', 2),\n",
       "  ('cookie', 11),\n",
       "  ('cooking', 1),\n",
       "  ('cool', 2),\n",
       "  ('coolest', 1),\n",
       "  ('coordination', 1),\n",
       "  ('cosy', 1),\n",
       "  ('couch', 1),\n",
       "  ('country', 1),\n",
       "  ('couple', 2),\n",
       "  ('coupon', 1),\n",
       "  ('cozy', 4),\n",
       "  ('create', 1),\n",
       "  ('creative', 1),\n",
       "  ('curious', 1),\n",
       "  ('custom', 2),\n",
       "  ('cute', 2),\n",
       "  ('date', 1),\n",
       "  ('dave', 1),\n",
       "  ('day', 4),\n",
       "  ('decor', 2),\n",
       "  ('definitely', 11),\n",
       "  ('delicious', 2),\n",
       "  ('describe', 7),\n",
       "  ('description', 2),\n",
       "  ('design', 2),\n",
       "  ('desk', 2),\n",
       "  ('detail', 1),\n",
       "  ('disappoint', 2),\n",
       "  ('discover', 1),\n",
       "  ('distraction', 1),\n",
       "  ('disturb', 1),\n",
       "  ('dog', 5),\n",
       "  ('door', 1),\n",
       "  ('doubt', 1),\n",
       "  ('downstairs', 1),\n",
       "  ('downtown', 5),\n",
       "  ('drawback', 1),\n",
       "  ('drive', 2),\n",
       "  ('driveway', 3),\n",
       "  ('duct', 1),\n",
       "  ('early', 2),\n",
       "  ('easy', 8),\n",
       "  ('eaterie', 1),\n",
       "  ('eclectic', 2),\n",
       "  ('enjoy', 14),\n",
       "  ('enjoyable', 2),\n",
       "  ('enough', 2),\n",
       "  ('entertainment', 1),\n",
       "  ('entire', 2),\n",
       "  ('entrance', 1),\n",
       "  ('environment', 1),\n",
       "  ('epitomise', 1),\n",
       "  ('equipped', 2),\n",
       "  ('especially', 1),\n",
       "  ('even', 5),\n",
       "  ('event', 1),\n",
       "  ('ever', 7),\n",
       "  ('everywhere', 1),\n",
       "  ('exactly', 8),\n",
       "  ('excellent', 6),\n",
       "  ('excited', 1),\n",
       "  ('expectation', 1),\n",
       "  ('experience', 11),\n",
       "  ('extra', 1),\n",
       "  ('extremely', 2),\n",
       "  ('fabulous', 1),\n",
       "  ('facility', 1),\n",
       "  ('family', 2),\n",
       "  ('fantastic', 3),\n",
       "  ('farmland', 1),\n",
       "  ('feel', 10),\n",
       "  ('fellow', 1),\n",
       "  ('fiance', 1),\n",
       "  ('field', 1),\n",
       "  ('fill', 1),\n",
       "  ('find', 6),\n",
       "  ('first', 9),\n",
       "  ('fitting', 1),\n",
       "  ('flat_screen', 1),\n",
       "  ('flier', 1),\n",
       "  ('folk', 1),\n",
       "  ('forever', 1),\n",
       "  ('frequently', 2),\n",
       "  ('fresh', 1),\n",
       "  ('friend', 5),\n",
       "  ('friendly', 7),\n",
       "  ('full', 2),\n",
       "  ('fun', 1),\n",
       "  ('furnish', 1),\n",
       "  ('future', 2),\n",
       "  ('garden', 3),\n",
       "  ('generous', 3),\n",
       "  ('get', 7),\n",
       "  ('giant', 1),\n",
       "  ('give', 4),\n",
       "  ('gladly', 1),\n",
       "  ('glider', 1),\n",
       "  ('go', 11),\n",
       "  ('golfer', 1),\n",
       "  ('good', 7),\n",
       "  ('gorgeous', 3),\n",
       "  ('gracious', 1),\n",
       "  ('greenway', 1),\n",
       "  ('greet', 3),\n",
       "  ('guest', 2),\n",
       "  ('guy', 1),\n",
       "  ('hand', 1),\n",
       "  ('hang', 1),\n",
       "  ('happy', 3),\n",
       "  ('hard', 3),\n",
       "  ('hate', 1),\n",
       "  ('helpful', 4),\n",
       "  ('hesitation', 1),\n",
       "  ('hideout', 1),\n",
       "  ('highly', 2),\n",
       "  ('hill', 1),\n",
       "  ('honestly', 1),\n",
       "  ('hope', 3),\n",
       "  ('hospitality', 2),\n",
       "  ('host', 22),\n",
       "  ('hotel', 3),\n",
       "  ('hour', 1),\n",
       "  ('hum', 1),\n",
       "  ('hustle', 1),\n",
       "  ('ideal', 1),\n",
       "  ('imagine', 1),\n",
       "  ('importantly', 1),\n",
       "  ('include', 2),\n",
       "  ('incredible', 4),\n",
       "  ('incredibly', 1),\n",
       "  ('information', 1),\n",
       "  ('informative', 1),\n",
       "  ('instantly', 1),\n",
       "  ('intake', 1),\n",
       "  ('invite', 1),\n",
       "  ('issue', 3),\n",
       "  ('jewel', 1),\n",
       "  ('join', 1),\n",
       "  ('justice', 2),\n",
       "  ('kind', 1),\n",
       "  ('kitchen', 3),\n",
       "  ('know', 1),\n",
       "  ('knowledge', 2),\n",
       "  ('land', 1),\n",
       "  ('landlord', 1),\n",
       "  ('large', 2),\n",
       "  ('last', 1),\n",
       "  ('late', 1),\n",
       "  ('later', 1),\n",
       "  ('launch', 1),\n",
       "  ('layout', 1),\n",
       "  ('leave', 5),\n",
       "  ('length', 1),\n",
       "  ('level', 1),\n",
       "  ('life', 1),\n",
       "  ('lighting', 1),\n",
       "  ('list', 2),\n",
       "  ('little', 6),\n",
       "  ('live', 2),\n",
       "  ('living', 2),\n",
       "  ('local', 2),\n",
       "  ('locate', 1),\n",
       "  ('long', 6),\n",
       "  ('look', 7),\n",
       "  ('lot', 1),\n",
       "  ('love', 9),\n",
       "  ('lovely', 6),\n",
       "  ('lucke', 1),\n",
       "  ('lucky', 2),\n",
       "  ('magical', 1),\n",
       "  ('maintain', 1),\n",
       "  ('make', 27),\n",
       "  ('mall', 2),\n",
       "  ('many', 4),\n",
       "  ('maria', 1),\n",
       "  ('mat', 1),\n",
       "  ('match', 2),\n",
       "  ('mattress', 2),\n",
       "  ('meal', 1),\n",
       "  ('meander', 1),\n",
       "  ('mecca', 1),\n",
       "  ('meet', 4),\n",
       "  ('minute', 9),\n",
       "  ('miss', 2),\n",
       "  ('model', 1),\n",
       "  ('modern', 1),\n",
       "  ('moment', 2),\n",
       "  ('money', 1),\n",
       "  ('month', 5),\n",
       "  ('move', 2),\n",
       "  ('much', 2),\n",
       "  ('music', 1),\n",
       "  ('musical', 1),\n",
       "  ('musician', 1),\n",
       "  ('name', 1),\n",
       "  ('nature', 2),\n",
       "  ('nearby', 1),\n",
       "  ('neat', 1),\n",
       "  ('need', 22),\n",
       "  ('neighborhood', 3),\n",
       "  ('new', 3),\n",
       "  ('next', 4),\n",
       "  ('nice', 6),\n",
       "  ('nicer', 1),\n",
       "  ('nicest', 1),\n",
       "  ('night', 1),\n",
       "  ('noise', 1),\n",
       "  ('noisy', 1),\n",
       "  ('note', 1),\n",
       "  ('number', 1),\n",
       "  ('occasional', 1),\n",
       "  ('oddity', 1),\n",
       "  ('offer', 2),\n",
       "  ('open', 1),\n",
       "  ('opportunity', 1),\n",
       "  ('outside', 2),\n",
       "  ('outstanding', 1),\n",
       "  ('overlook', 3),\n",
       "  ('owner', 1),\n",
       "  ('paradise', 1),\n",
       "  ('park', 2),\n",
       "  ('parking', 1),\n",
       "  ('part', 1),\n",
       "  ('past', 1),\n",
       "  ('pay', 1),\n",
       "  ('peace', 1),\n",
       "  ('peaceful', 1),\n",
       "  ('peacefulness', 1),\n",
       "  ('people', 7),\n",
       "  ('perfect', 9),\n",
       "  ('permission', 1),\n",
       "  ('person', 2),\n",
       "  ('pet', 2),\n",
       "  ('phenomenal', 1),\n",
       "  ('phone', 1),\n",
       "  ('photo', 1),\n",
       "  ('picture', 1),\n",
       "  ('place', 4),\n",
       "  ('plan', 3),\n",
       "  ('plane', 1),\n",
       "  ('planning', 1),\n",
       "  ('play', 2),\n",
       "  ('player', 1),\n",
       "  ('pleasant', 2),\n",
       "  ('pleased', 1),\n",
       "  ('pleasurable', 1),\n",
       "  ('pleasure', 1),\n",
       "  ('plenty', 1),\n",
       "  ('plot', 1),\n",
       "  ('politely', 1),\n",
       "  ('prefer', 1),\n",
       "  ('preferred', 1),\n",
       "  ('pretty', 1),\n",
       "  ('price', 1),\n",
       "  ('privacy', 6),\n",
       "  ('private', 1),\n",
       "  ('probably', 1),\n",
       "  ('problem', 1),\n",
       "  ('project', 1),\n",
       "  ('property', 9),\n",
       "  ('provide', 4),\n",
       "  ('provision', 1),\n",
       "  ('proximity', 1),\n",
       "  ('pull', 2),\n",
       "  ('put', 1),\n",
       "  ('question', 2),\n",
       "  ('quick', 1),\n",
       "  ('quickly', 1),\n",
       "  ('quiet', 8),\n",
       "  ('quirky', 1),\n",
       "  ('quite', 3),\n",
       "  ('rain', 1),\n",
       "  ('ray', 1),\n",
       "  ('readily', 1),\n",
       "  ('ready', 1),\n",
       "  ('real', 1),\n",
       "  ('really', 6),\n",
       "  ('recommend', 13),\n",
       "  ('recommendation', 1),\n",
       "  ('recreation', 2),\n",
       "  ('relax', 4),\n",
       "  ('relaxed', 1),\n",
       "  ('remarkable', 1),\n",
       "  ('renovate', 1),\n",
       "  ('rent', 1),\n",
       "  ('reservation', 1),\n",
       "  ('respect', 3),\n",
       "  ('restaurant', 2),\n",
       "  ('retreat', 2),\n",
       "  ('retro', 1),\n",
       "  ('return', 4),\n",
       "  ('review', 1),\n",
       "  ('ride', 1),\n",
       "  ('ridge', 1),\n",
       "  ('right', 2),\n",
       "  ('river', 11),\n",
       "  ('road', 2),\n",
       "  ('rock', 1),\n",
       "  ('room', 2),\n",
       "  ('running', 1),\n",
       "  ('rural', 1),\n",
       "  ('satisfied', 1),\n",
       "  ('say', 1),\n",
       "  ('scene', 1),\n",
       "  ('scenery', 1),\n",
       "  ('second', 1),\n",
       "  ('see', 5),\n",
       "  ('seek', 1),\n",
       "  ('series', 1),\n",
       "  ('serve', 1),\n",
       "  ('settle', 1),\n",
       "  ('shame', 1),\n",
       "  ('share', 3),\n",
       "  ('shelf', 1),\n",
       "  ('shopping', 1),\n",
       "  ('show', 6),\n",
       "  ('shower', 1),\n",
       "  ('sightseeing', 1),\n",
       "  ('similar', 1),\n",
       "  ('sit', 3),\n",
       "  ('site', 1),\n",
       "  ('situate', 1),\n",
       "  ('size', 2),\n",
       "  ('sleep', 1),\n",
       "  ('small', 3),\n",
       "  ('snd', 1),\n",
       "  ('space', 11),\n",
       "  ('spacious', 2),\n",
       "  ('special', 1),\n",
       "  ('spectacular', 4),\n",
       "  ('spend', 2),\n",
       "  ('spiritual', 1),\n",
       "  ('spot', 1),\n",
       "  ('spotless', 1),\n",
       "  ('stay', 12),\n",
       "  ('stayed', 1),\n",
       "  ('still', 2),\n",
       "  ('stone_throw', 1),\n",
       "  ('stop', 1),\n",
       "  ('store', 1),\n",
       "  ('story', 1),\n",
       "  ('stuff', 1),\n",
       "  ('stunning', 1),\n",
       "  ('suggestion', 2),\n",
       "  ('super', 4),\n",
       "  ('supply', 1),\n",
       "  ('sure', 2),\n",
       "  ('surprisingly', 1),\n",
       "  ('surround', 1),\n",
       "  ('surrounding', 1),\n",
       "  ('swing', 2),\n",
       "  ('take', 2),\n",
       "  ('taken_care', 1),\n",
       "  ('talk', 4),\n",
       "  ('tell', 1),\n",
       "  ('term', 1),\n",
       "  ('terrific', 1),\n",
       "  ('texte', 1),\n",
       "  ('thank', 3),\n",
       "  ('thankful', 1),\n",
       "  ('thing', 2),\n",
       "  ('think', 1),\n",
       "  ('thoroughly', 2),\n",
       "  ('thoughtful', 1),\n",
       "  ('thoughtfully', 1),\n",
       "  ('threaten', 1),\n",
       "  ('time', 10),\n",
       "  ('together', 1),\n",
       "  ('top', 1),\n",
       "  ('torn', 1),\n",
       "  ('touch', 3),\n",
       "  ('town', 2),\n",
       "  ('trail', 3),\n",
       "  ('tranquility', 1),\n",
       "  ('travel', 3),\n",
       "  ('trip', 4),\n",
       "  ('trouble_finde', 1),\n",
       "  ('true', 1),\n",
       "  ('truly', 2),\n",
       "  ('try', 1),\n",
       "  ('tuck', 1),\n",
       "  ('tug', 1),\n",
       "  ('tv', 1),\n",
       "  ('twin', 1),\n",
       "  ('umbrella', 1),\n",
       "  ('unbelievable', 1),\n",
       "  ('unexpected', 1),\n",
       "  ('unidentified', 1),\n",
       "  ('unique', 2),\n",
       "  ('upgrade', 1),\n",
       "  ('upon_arrival', 1),\n",
       "  ('upstairs', 2),\n",
       "  ('use', 5),\n",
       "  ('vacation', 2),\n",
       "  ('value', 2),\n",
       "  ('vast', 1),\n",
       "  ('vent', 1),\n",
       "  ('venture', 1),\n",
       "  ('venue', 1),\n",
       "  ('view', 41),\n",
       "  ('visit', 4),\n",
       "  ('vista', 1),\n",
       "  ('wait', 4),\n",
       "  ('walk', 4),\n",
       "  ('walkway', 1),\n",
       "  ('want', 6),\n",
       "  ('warm', 2),\n",
       "  ('washer', 1),\n",
       "  ('water', 1),\n",
       "  ('way', 3),\n",
       "  ('website_hidden', 1),\n",
       "  ('week', 2),\n",
       "  ('weekend', 2),\n",
       "  ('weird', 1),\n",
       "  ('welcome', 8),\n",
       "  ('welcoming', 1),\n",
       "  ('well', 12),\n",
       "  ('whenever', 2),\n",
       "  ('wherever', 1),\n",
       "  ('whole', 2),\n",
       "  ('wife', 1),\n",
       "  ('wifi', 1),\n",
       "  ('willing', 1),\n",
       "  ('wish', 3),\n",
       "  ('wonderful', 20),\n",
       "  ('work', 5),\n",
       "  ('world', 1),\n",
       "  ('wrong', 1),\n",
       "  ('yard', 9),\n",
       "  ('year', 3),\n",
       "  ('yet', 2)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,        # Replace with gensim.models.ldamodel.LdaModel()\n",
    "                       id2word=id2word,\n",
    "                       num_topics=8, #number of topics to identify\n",
    "                       random_state=100,\n",
    "                       #update_every=1,                          #Add back in with LdaModel\n",
    "                       chunksize=100, #number of documents to pass per chunk\n",
    "                       passes=10, #number of training passes\n",
    "                       #alpha='auto',                            #Add back in with LdaModel\n",
    "                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.039*\"clean\" + 0.023*\"space\" + 0.020*\"recommend\" + 0.019*\"comfortable\" + '\n",
      "  '0.018*\"definitely\" + 0.016*\"perfect\" + 0.014*\"love\" + 0.014*\"need\" + '\n",
      "  '0.012*\"super\" + 0.012*\"close\"'),\n",
      " (1,\n",
      "  '0.027*\"walk\" + 0.024*\"restaurant\" + 0.017*\"neighborhood\" + 0.015*\"perfect\" '\n",
      "  '+ 0.014*\"clean\" + 0.014*\"distance\" + 0.013*\"downtown\" + 0.013*\"comfortable\" '\n",
      "  '+ 0.013*\"shop\" + 0.012*\"recommend\"'),\n",
      " (2,\n",
      "  '0.045*\"room\" + 0.026*\"clean\" + 0.020*\"nice\" + 0.013*\"downtown\" + '\n",
      "  '0.013*\"comfortable\" + 0.013*\"friendly\" + 0.012*\"good\" + 0.012*\"recommend\" + '\n",
      "  '0.012*\"bathroom\" + 0.010*\"stay\"'),\n",
      " (3,\n",
      "  '0.015*\"make\" + 0.014*\"need\" + 0.013*\"recommend\" + 0.013*\"downtown\" + '\n",
      "  '0.012*\"clean\" + 0.012*\"host\" + 0.011*\"comfortable\" + 0.010*\"definitely\" + '\n",
      "  '0.010*\"perfect\" + 0.009*\"time\"'),\n",
      " (4,\n",
      "  '0.026*\"walk\" + 0.023*\"clean\" + 0.023*\"apartment\" + 0.012*\"recommend\" + '\n",
      "  '0.012*\"definitely\" + 0.011*\"distance\" + 0.011*\"condo\" + 0.010*\"perfect\" + '\n",
      "  '0.010*\"nice\" + 0.010*\"downtown\"'),\n",
      " (5,\n",
      "  '0.012*\"make\" + 0.011*\"feel\" + 0.011*\"beautiful\" + 0.011*\"love\" + '\n",
      "  '0.010*\"enjoy\" + 0.010*\"need\" + 0.010*\"comfortable\" + 0.009*\"time\" + '\n",
      "  '0.009*\"wonderful\" + 0.009*\"stay\"'),\n",
      " (6,\n",
      "  '0.022*\"group\" + 0.015*\"space\" + 0.015*\"recommend\" + 0.014*\"clean\" + '\n",
      "  '0.013*\"perfect\" + 0.012*\"downtown\" + 0.012*\"room\" + 0.011*\"large\" + '\n",
      "  '0.011*\"definitely\" + 0.010*\"time\"'),\n",
      " (7,\n",
      "  '0.020*\"clean\" + 0.012*\"downtown\" + 0.012*\"apartment\" + 0.012*\"get\" + '\n",
      "  '0.011*\"nice\" + 0.011*\"close\" + 0.011*\"good\" + 0.010*\"night\" + 0.010*\"need\" '\n",
      "  '+ 0.009*\"bed\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 Keywords in each grouped Topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.569990494841519\n",
      "\n",
      "Coherence Score:  0.28406596548923907\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score - Likely more helpful. Takes a while to run.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual for Viewing each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda_non_gentrifying.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipping step 17 about finding the best number of topics - Tim recommends 8-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant Topic in each Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-57075f752869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "\n",
    "for pdf, doc in tqdm(zip(pdfs, docs)):\n",
    "    bow = corpora.Dictionary(data_lemmatized)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    for res in topics:\n",
    "        topic, pct = res\n",
    "        contents.append({'pdf': pdf, 'topic': topic, 'percent': pct})\n",
    "\n",
    "topics = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get main topic in each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_topic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most representative Reviews for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "\n",
    "        file = topics[topics.topic == i].sort_values('percent', ascending = False).head().iloc[j, 0]\n",
    "\n",
    "        cmd = f'cp {file} representative_docs/topic_{i + 1}/.'\n",
    "\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA - Location Review of 7 or Less\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = airbnb[airbnb['review_scores_location']<=7].comments_concatenated.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review break symbols\n",
    "data = [re.sub(\"\\\\r\\\\n\", \"\", comment) for comment in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence to words, removing uneeded words/characters\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tagged_words = nltk.tag.pos_tag(sentence.split()) \n",
    "        no_names = [word for word,tag in tagged_words if tag != 'NNP' and tag != 'NNPS'] # Remove proper nouns\n",
    "        yield(gensim.utils.simple_preprocess(str(no_names), deacc=True)) #Clean and remove punctuation\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words,\n",
    "                               min_count=5,\n",
    "                               threshold=100)#,  # higher threshold fewer phrases.\n",
    "                               #connector_words=phrases.ENGLISH_CONNECTOR_WORDS) ***I think I need to download this.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)#, connector_words=phrases.ENGLISH_CONNECTOR_WORDS)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Test trigram on first review\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'stay', 'place', 'location', 'home', 'house', 'host', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bang'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to view a single word within the corpus\n",
    "id2word[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('bachelorette', 1),\n",
       "  ('charming', 1),\n",
       "  ('chic', 1),\n",
       "  ('comfortable', 1),\n",
       "  ('country', 1),\n",
       "  ('eachother', 1),\n",
       "  ('fit', 1),\n",
       "  ('girl', 1),\n",
       "  ('impeccable', 1),\n",
       "  ('key', 1),\n",
       "  ('low', 1),\n",
       "  ('people', 1),\n",
       "  ('property', 1),\n",
       "  ('shape', 1),\n",
       "  ('spot', 1),\n",
       "  ('top', 1),\n",
       "  ('weekend', 1)]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,        # Replace with gensim.models.ldamodel.LdaModel()\n",
    "                       id2word=id2word,\n",
    "                       num_topics=8, #number of topics to identify\n",
    "                       random_state=100,\n",
    "                       #update_every=1,                          #Add back in with LdaModel\n",
    "                       chunksize=100, #number of documents to pass per chunk\n",
    "                       passes=10, #number of training passes\n",
    "                       #alpha='auto',                            #Add back in with LdaModel\n",
    "                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"feel\" + 0.010*\"neighborhood\" + 0.010*\"hour\" + 0.010*\"bad\" + '\n",
      "  '0.010*\"back\" + 0.008*\"issue\" + 0.008*\"building\" + 0.007*\"definitely\" + '\n",
      "  '0.006*\"area\" + 0.006*\"come\"'),\n",
      " (1,\n",
      "  '0.015*\"clean\" + 0.012*\"get\" + 0.012*\"area\" + 0.011*\"downtown\" + '\n",
      "  '0.010*\"apartment\" + 0.010*\"nice\" + 0.010*\"neighborhood\" + 0.009*\"tell\" + '\n",
      "  '0.009*\"book\" + 0.009*\"safe\"'),\n",
      " (2,\n",
      "  '0.020*\"clean\" + 0.013*\"downtown\" + 0.011*\"group\" + 0.011*\"night\" + '\n",
      "  '0.010*\"nice\" + 0.008*\"get\" + 0.008*\"well\" + 0.008*\"easy\" + 0.007*\"make\" + '\n",
      "  '0.007*\"comfortable\"'),\n",
      " (3,\n",
      "  '0.023*\"cottage\" + 0.014*\"enjoy\" + 0.012*\"clean\" + 0.011*\"perfect\" + '\n",
      "  '0.011*\"love\" + 0.009*\"get\" + 0.008*\"look\" + 0.008*\"little\" + 0.008*\"need\" + '\n",
      "  '0.008*\"cute\"'),\n",
      " (4,\n",
      "  '0.018*\"nice\" + 0.016*\"clean\" + 0.016*\"area\" + 0.014*\"downtown\" + '\n",
      "  '0.013*\"neighborhood\" + 0.012*\"feel\" + 0.011*\"group\" + 0.010*\"amazing\" + '\n",
      "  '0.009*\"really\" + 0.009*\"get\"'),\n",
      " (5,\n",
      "  '0.013*\"clean\" + 0.013*\"get\" + 0.011*\"refund\" + 0.007*\"lack\" + 0.007*\"break\" '\n",
      "  '+ 0.007*\"money\" + 0.007*\"completely\" + 0.007*\"toilet\" + 0.006*\"friend\" + '\n",
      "  '0.006*\"answer\"'),\n",
      " (6,\n",
      "  '0.021*\"clean\" + 0.015*\"unit\" + 0.009*\"get\" + 0.009*\"ask\" + 0.008*\"know\" + '\n",
      "  '0.008*\"night\" + 0.007*\"even\" + 0.007*\"construction\" + 0.007*\"dirty\" + '\n",
      "  '0.007*\"move\"'),\n",
      " (7,\n",
      "  '0.018*\"area\" + 0.012*\"clean\" + 0.010*\"bed\" + 0.010*\"come\" + 0.009*\"check\" + '\n",
      "  '0.008*\"room\" + 0.008*\"call\" + 0.008*\"follow\" + 0.007*\"nice\" + 0.007*\"safe\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 Keywords in each grouped Topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.028445041980549\n",
      "\n",
      "Coherence Score:  0.35090525787610866\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score - Likely more helpful. Takes a while to run.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual for Viewing each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda_bad_location_score.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipping step 17 about finding the best number of topics - Tim recommends 8-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant Topic in each Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-57075f752869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "\n",
    "for pdf, doc in tqdm(zip(pdfs, docs)):\n",
    "    bow = corpora.Dictionary(data_lemmatized)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    for res in topics:\n",
    "        topic, pct = res\n",
    "        contents.append({'pdf': pdf, 'topic': topic, 'percent': pct})\n",
    "\n",
    "topics = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get main topic in each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_topic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most representative Reviews for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "\n",
    "        file = topics[topics.topic == i].sort_values('percent', ascending = False).head().iloc[j, 0]\n",
    "\n",
    "        cmd = f'cp {file} representative_docs/topic_{i + 1}/.'\n",
    "\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
