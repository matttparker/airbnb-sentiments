{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mattparker/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or command prompt:\n",
    "#python3 -m spacy download en\n",
    "\n",
    "# Packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Import stopwords and other word packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # as gensimvis  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, coo_matrix\n",
    "\n",
    "\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv('../data/airbnb_gentrification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments_concatenated</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>description</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>property_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>amenities</th>\n",
       "      <th>minimum_nights_avg_ntm</th>\n",
       "      <th>maximum_nights_avg_ntm</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>house_price_2021-01-31</th>\n",
       "      <th>house_pct_change</th>\n",
       "      <th>rentals_2021-01-31</th>\n",
       "      <th>rental_price_pct_change</th>\n",
       "      <th>new_restaurants</th>\n",
       "      <th>available_beer</th>\n",
       "      <th>str_permits_2020</th>\n",
       "      <th>str_permits_growth</th>\n",
       "      <th>crimes</th>\n",
       "      <th>total_pop_2010</th>\n",
       "      <th>total_pop_2019</th>\n",
       "      <th>total_pop_change</th>\n",
       "      <th>total_pop_pct_change</th>\n",
       "      <th>pop_over25_2010</th>\n",
       "      <th>pop_over25_2019</th>\n",
       "      <th>pop_over25_change</th>\n",
       "      <th>pop_over25_pcg_change</th>\n",
       "      <th>total_households_2010</th>\n",
       "      <th>total_households_2019</th>\n",
       "      <th>total_households_change</th>\n",
       "      <th>total_households_pct_change</th>\n",
       "      <th>white_pct_2010</th>\n",
       "      <th>white_pct_2019</th>\n",
       "      <th>white_value_change</th>\n",
       "      <th>white_pct_change</th>\n",
       "      <th>bach_pct_2010</th>\n",
       "      <th>bach_pct_2019</th>\n",
       "      <th>bach_value_change</th>\n",
       "      <th>bach_pct_change</th>\n",
       "      <th>rent_pct_2010</th>\n",
       "      <th>rent_pct_2019</th>\n",
       "      <th>rent_value_change</th>\n",
       "      <th>renter_pct_change</th>\n",
       "      <th>median_hhi_2010</th>\n",
       "      <th>median_hhi_2019</th>\n",
       "      <th>median_hhi_value_change</th>\n",
       "      <th>median_hhi_pct_change</th>\n",
       "      <th>poverty_pct_2010</th>\n",
       "      <th>poverty_pct_2019</th>\n",
       "      <th>poverty_value_change</th>\n",
       "      <th>poverty_pct_change</th>\n",
       "      <th>gentrifying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6422</td>\n",
       "      <td>I can't say enough about how wonderful it was ...</td>\n",
       "      <td>Nashville Charm</td>\n",
       "      <td>12172</td>\n",
       "      <td>36.17315</td>\n",
       "      <td>-86.73581</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>674</td>\n",
       "      <td>4.69</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>https://www.airbnb.com/rooms/6422</td>\n",
       "      <td>30 day or more rental during COVID. Show COVID...</td>\n",
       "      <td>Historic East Nashville is home to many new an...</td>\n",
       "      <td>2009-04-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private room in house</td>\n",
       "      <td>2</td>\n",
       "      <td>1 private bath</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[\"Hair dryer\", \"Bathtub\", \"Lock on bedroom doo...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37206.0</td>\n",
       "      <td>4.703701e+10</td>\n",
       "      <td>412476.0</td>\n",
       "      <td>38.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>-444.0</td>\n",
       "      <td>-0.174528</td>\n",
       "      <td>1703.0</td>\n",
       "      <td>1639.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-0.037581</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>-214.0</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>0.657626</td>\n",
       "      <td>0.940952</td>\n",
       "      <td>0.283327</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.585723</td>\n",
       "      <td>0.177032</td>\n",
       "      <td>0.43317</td>\n",
       "      <td>0.320175</td>\n",
       "      <td>0.240821</td>\n",
       "      <td>-0.079355</td>\n",
       "      <td>-0.247848</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>91643.0</td>\n",
       "      <td>45643.0</td>\n",
       "      <td>0.992239</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.037736</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id                              comments_concatenated             name  host_id  latitude  longitude  price  minimum_nights  number_of_reviews  reviews_per_month  calculated_host_listings_count  availability_365                        listing_url                                        description                              neighborhood_overview  host_since  host_listings_count          property_type  accommodates  bathrooms_text  bedrooms  beds                                          amenities  minimum_nights_avg_ntm  maximum_nights_avg_ntm  review_scores_rating  review_scores_accuracy  review_scores_cleanliness  review_scores_checkin  review_scores_communication  review_scores_location  review_scores_value  zip_code         GEOID  house_price_2021-01-31  house_pct_change  rentals_2021-01-31  rental_price_pct_change  new_restaurants  available_beer  str_permits_2020  str_permits_growth  crimes  total_pop_2010  total_pop_2019  total_pop_change  total_pop_pct_change  \\\n",
       "0        6422  I can't say enough about how wonderful it was ...  Nashville Charm    12172  36.17315  -86.73581     40              30                674               4.69                               1               267  https://www.airbnb.com/rooms/6422  30 day or more rental during COVID. Show COVID...  Historic East Nashville is home to many new an...  2009-04-03                  0.0  Private room in house             2  1 private bath       2.0   3.0  [\"Hair dryer\", \"Bathtub\", \"Lock on bedroom doo...                    30.0                   365.0                  99.0                    10.0                       10.0                   10.0                         10.0                    10.0                 10.0   37206.0  4.703701e+10                412476.0             38.31                 NaN                      NaN              1.0             2.0             114.0               114.0  1165.0          2544.0          2100.0            -444.0             -0.174528   \n",
       "\n",
       "   pop_over25_2010  pop_over25_2019  pop_over25_change  pop_over25_pcg_change  total_households_2010  total_households_2019  total_households_change  total_households_pct_change  white_pct_2010  white_pct_2019  white_value_change  white_pct_change  bach_pct_2010  bach_pct_2019  bach_value_change  bach_pct_change  rent_pct_2010  rent_pct_2019  rent_value_change  renter_pct_change  median_hhi_2010  median_hhi_2019  median_hhi_value_change  median_hhi_pct_change  poverty_pct_2010  poverty_pct_2019  poverty_value_change  poverty_pct_change  gentrifying  \n",
       "0           1703.0           1639.0              -64.0              -0.037581                 1140.0                  926.0                   -214.0                    -0.187719        0.657626        0.940952            0.283327          0.430833       0.408691       0.585723           0.177032          0.43317       0.320175       0.240821          -0.079355          -0.247848          46000.0          91643.0                  45643.0               0.992239              10.6              10.2                  -0.4           -0.037736        False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5205, 76)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = airbnb[airbnb['comments_concatenated'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_nlp = airbnb[['listing_id', 'comments_concatenated', 'gentrifying']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_nlp.comments_concatenated.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists with training and testing apns\n",
    "train_listings, test_listings = tts(airbnb_nlp['listing_id'].to_list(), \n",
    "                                    random_state = 42, \n",
    "                                    stratify=airbnb_nlp['gentrifying'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3903"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes from the lists of apns\n",
    "airbnb_train = airbnb_nlp[airbnb_nlp['listing_id'].isin(train_listings)].sort_values('listing_id')\n",
    "airbnb_test = airbnb_nlp[airbnb_nlp['listing_id'].isin(test_listings)].sort_values('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the y_train and y_test dataframes from the lists of apns\n",
    "y_train = airbnb_nlp['gentrifying']\n",
    "y_test = airbnb_nlp['gentrifying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    0.755829\n",
      "True     0.244171\n",
      "Name: gentrifying, dtype: float64\n",
      "False    0.756341\n",
      "True     0.243659\n",
      "Name: gentrifying, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(airbnb_train.gentrifying.value_counts(normalize=True))\n",
    "print(airbnb_test.gentrifying.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Blog - Machine Learning Plus\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = airbnb_train.comments_concatenated.iloc[0:200].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review break symbols\n",
    "data = [re.sub(\"\\\\r\\\\n\", \"\", comment) for comment in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set to Markdown for Safety\n",
    "\n",
    "#Tokenize each sentence to words, remove punctuations and unnecessary characters\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words_with_names = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence to words, removing uneeded words/characters\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tagged_words = nltk.tag.pos_tag(sentence.split()) \n",
    "        no_names = [word for word,tag in tagged_words if tag != 'NNP' and tag != 'NNPS'] # Remove proper nouns\n",
    "        yield(gensim.utils.simple_preprocess(str(no_names), deacc=True)) #Clean and remove punctuation\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words,\n",
    "                               min_count=5,\n",
    "                               threshold=100)#,  # higher threshold fewer phrases.\n",
    "                               #connector_words=phrases.ENGLISH_CONNECTOR_WORDS) ***I think I need to download this.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)#, connector_words=phrases.ENGLISH_CONNECTOR_WORDS)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Test trigram on first review\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'stay', 'place', 'location', 'home', 'house', 'host', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deu'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to view a single word within the corpus\n",
    "id2word[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abend', 1),\n",
       "  ('aber', 1),\n",
       "  ('able', 23),\n",
       "  ('abound', 1),\n",
       "  ('aboyait', 1),\n",
       "  ('absente', 1),\n",
       "  ('absolument', 1),\n",
       "  ('absolute', 7),\n",
       "  ('absolute_pleasure', 3),\n",
       "  ('absolutely', 45),\n",
       "  ('access', 15),\n",
       "  ('accessible', 7),\n",
       "  ('acceuillante', 1),\n",
       "  ('acclimate', 1),\n",
       "  ('accom', 1),\n",
       "  ('accommodate', 37),\n",
       "  ('accommodating', 1),\n",
       "  ('accommodation', 23),\n",
       "  ('accomodate', 1),\n",
       "  ('accomodation', 4),\n",
       "  ('accompagne', 1),\n",
       "  ('accompany', 1),\n",
       "  ('accueil', 2),\n",
       "  ('accueilli', 1),\n",
       "  ('accueillir', 1),\n",
       "  ('accurate', 1),\n",
       "  ('acknowledge', 1),\n",
       "  ('acogedore', 1),\n",
       "  ('acommodation', 1),\n",
       "  ('act', 2),\n",
       "  ('activism', 1),\n",
       "  ('activity', 5),\n",
       "  ('actually', 7),\n",
       "  ('add', 5),\n",
       "  ('addition', 2),\n",
       "  ('adequate', 2),\n",
       "  ('adjacent', 5),\n",
       "  ('adjoining', 2),\n",
       "  ('adjust', 2),\n",
       "  ('adopt', 1),\n",
       "  ('adorable', 27),\n",
       "  ('adoravel', 1),\n",
       "  ('adore', 1),\n",
       "  ('adresse', 1),\n",
       "  ('advantage', 1),\n",
       "  ('adventure', 4),\n",
       "  ('adventurous', 1),\n",
       "  ('advertize', 1),\n",
       "  ('advice', 17),\n",
       "  ('advise', 1),\n",
       "  ('affect', 1),\n",
       "  ('affectionate', 2),\n",
       "  ('affordability', 1),\n",
       "  ('affordable', 3),\n",
       "  ('afternoon', 3),\n",
       "  ('agenda', 1),\n",
       "  ('aggressively', 1),\n",
       "  ('agin', 1),\n",
       "  ('ago', 1),\n",
       "  ('agreable', 1),\n",
       "  ('agree', 3),\n",
       "  ('ainda', 1),\n",
       "  ('air', 5),\n",
       "  ('airbnb', 27),\n",
       "  ('airbnbs', 1),\n",
       "  ('airport', 7),\n",
       "  ('airy', 2),\n",
       "  ('ajudar', 1),\n",
       "  ('al', 1),\n",
       "  ('alcove', 1),\n",
       "  ('algo', 1),\n",
       "  ('alle', 2),\n",
       "  ('aller', 1),\n",
       "  ('allergic', 2),\n",
       "  ('allergy', 1),\n",
       "  ('allow', 5),\n",
       "  ('almost', 7),\n",
       "  ('alojamiento', 1),\n",
       "  ('alojarme', 1),\n",
       "  ('alone', 3),\n",
       "  ('alot', 1),\n",
       "  ('already', 3),\n",
       "  ('also', 62),\n",
       "  ('alternative', 1),\n",
       "  ('always', 19),\n",
       "  ('amazed', 1),\n",
       "  ('amazing', 66),\n",
       "  ('amazingly', 1),\n",
       "  ('ambassador', 2),\n",
       "  ('amenity', 18),\n",
       "  ('american', 4),\n",
       "  ('amount', 2),\n",
       "  ('ample', 2),\n",
       "  ('animal', 16),\n",
       "  ('anna', 1),\n",
       "  ('answer', 6),\n",
       "  ('anwhere', 1),\n",
       "  ('anxious', 1),\n",
       "  ('anytime', 1),\n",
       "  ('anywhere', 1),\n",
       "  ('anywhere_else', 1),\n",
       "  ('apart', 2),\n",
       "  ('apartment', 3),\n",
       "  ('apply', 1),\n",
       "  ('appoint', 2),\n",
       "  ('apporter', 1),\n",
       "  ('apportion', 1),\n",
       "  ('appreciate', 11),\n",
       "  ('appreciated', 4),\n",
       "  ('apprehensive', 1),\n",
       "  ('approximately', 1),\n",
       "  ('aptly', 1),\n",
       "  ('architecture', 1),\n",
       "  ('area', 71),\n",
       "  ('armed', 1),\n",
       "  ('around', 8),\n",
       "  ('arrange', 1),\n",
       "  ('arrangement', 1),\n",
       "  ('array', 1),\n",
       "  ('arrival', 15),\n",
       "  ('arrive', 40),\n",
       "  ('arrivee', 1),\n",
       "  ('arropado', 1),\n",
       "  ('arstsy', 1),\n",
       "  ('art', 3),\n",
       "  ('artistic', 2),\n",
       "  ('artsy', 5),\n",
       "  ('asia', 1),\n",
       "  ('ask', 26),\n",
       "  ('aspect', 2),\n",
       "  ('assiette', 1),\n",
       "  ('assist', 2),\n",
       "  ('assistance', 1),\n",
       "  ('assortment', 1),\n",
       "  ('atmosphere', 6),\n",
       "  ('attach', 1),\n",
       "  ('attend', 2),\n",
       "  ('attendait', 1),\n",
       "  ('attending_conference', 2),\n",
       "  ('attentive', 5),\n",
       "  ('attraction', 3),\n",
       "  ('au', 1),\n",
       "  ('auf', 2),\n",
       "  ('aunt', 2),\n",
       "  ('auparavant', 1),\n",
       "  ('aura', 1),\n",
       "  ('aussie', 1),\n",
       "  ('automated_poste', 4),\n",
       "  ('autumn', 1),\n",
       "  ('availability', 3),\n",
       "  ('available', 13),\n",
       "  ('avait', 1),\n",
       "  ('avoir', 1),\n",
       "  ('awake', 2),\n",
       "  ('aware', 1),\n",
       "  ('away', 47),\n",
       "  ('awesome', 20),\n",
       "  ('awkward', 1),\n",
       "  ('awkwardness', 1),\n",
       "  ('ayant', 1),\n",
       "  ('ayudado', 1),\n",
       "  ('baby', 4),\n",
       "  ('back', 92),\n",
       "  ('backpack', 1),\n",
       "  ('backyard', 7),\n",
       "  ('bad', 2),\n",
       "  ('bag', 1),\n",
       "  ('bake', 2),\n",
       "  ('baked', 2),\n",
       "  ('baked_good', 1),\n",
       "  ('balance', 4),\n",
       "  ('banana', 1),\n",
       "  ('bar', 16),\n",
       "  ('bargain', 1),\n",
       "  ('bark', 1),\n",
       "  ('barrio', 1),\n",
       "  ('base', 3),\n",
       "  ('basement', 1),\n",
       "  ('basic', 2),\n",
       "  ('bath', 4),\n",
       "  ('bathe', 1),\n",
       "  ('bathroom', 43),\n",
       "  ('bday', 1),\n",
       "  ('be', 1),\n",
       "  ('beat', 9),\n",
       "  ('beautiful', 117),\n",
       "  ('beautifull', 1),\n",
       "  ('beautifully', 6),\n",
       "  ('bed', 54),\n",
       "  ('bedroom', 26),\n",
       "  ('bedsheet', 1),\n",
       "  ('beforehand', 1),\n",
       "  ('begin', 2),\n",
       "  ('beginning', 1),\n",
       "  ('begrusste', 1),\n",
       "  ('behave', 2),\n",
       "  ('behind', 1),\n",
       "  ('behold', 1),\n",
       "  ('believe', 2),\n",
       "  ('belonging', 1),\n",
       "  ('beste', 1),\n",
       "  ('bicycle', 3),\n",
       "  ('big', 20),\n",
       "  ('bij', 1),\n",
       "  ('bike', 4),\n",
       "  ('biking', 6),\n",
       "  ('biscuit', 7),\n",
       "  ('bit', 12),\n",
       "  ('blackberry', 1),\n",
       "  ('block', 4),\n",
       "  ('blood', 1),\n",
       "  ('blow', 2),\n",
       "  ('boarder', 1),\n",
       "  ('boisterous', 1),\n",
       "  ('bon', 1),\n",
       "  ('bonne', 1),\n",
       "  ('bonus', 7),\n",
       "  ('book', 27),\n",
       "  ('booking', 2),\n",
       "  ('bookshelf', 1),\n",
       "  ('border', 1),\n",
       "  ('bother', 1),\n",
       "  ('bottle', 1),\n",
       "  ('bounty', 1),\n",
       "  ('bourratif', 1),\n",
       "  ('box', 1),\n",
       "  ('boyfriend', 3),\n",
       "  ('branche', 1),\n",
       "  ('braucht', 1),\n",
       "  ('bread', 4),\n",
       "  ('breakfast', 61),\n",
       "  ('breathe', 1),\n",
       "  ('breeze', 5),\n",
       "  ('brew', 1),\n",
       "  ('brief', 4),\n",
       "  ('briefly', 2),\n",
       "  ('bright', 12),\n",
       "  ('brilliant', 4),\n",
       "  ('bring', 4),\n",
       "  ('broadway', 3),\n",
       "  ('brother', 1),\n",
       "  ('brownie', 2),\n",
       "  ('browse', 1),\n",
       "  ('brunch', 9),\n",
       "  ('bunch', 1),\n",
       "  ('bungalow', 2),\n",
       "  ('bus', 32),\n",
       "  ('bus_stop', 20),\n",
       "  ('business', 1),\n",
       "  ('buss', 1),\n",
       "  ('bustle', 1),\n",
       "  ('bustling', 1),\n",
       "  ('busy', 1),\n",
       "  ('button', 1),\n",
       "  ('buy', 1),\n",
       "  ('cable', 1),\n",
       "  ('cafe', 8),\n",
       "  ('cake', 4),\n",
       "  ('calfeutree', 1),\n",
       "  ('call', 4),\n",
       "  ('calme', 1),\n",
       "  ('camaraderie', 1),\n",
       "  ('cancel', 6),\n",
       "  ('cant_wait', 1),\n",
       "  ('car', 16),\n",
       "  ('care', 10),\n",
       "  ('cared', 1),\n",
       "  ('caring', 2),\n",
       "  ('carinosos', 1),\n",
       "  ('carry', 1),\n",
       "  ('casa', 4),\n",
       "  ('case', 2),\n",
       "  ('cat', 50),\n",
       "  ('catch', 5),\n",
       "  ('celebration', 1),\n",
       "  ('center', 2),\n",
       "  ('centre', 6),\n",
       "  ('centro', 1),\n",
       "  ('ceremony', 1),\n",
       "  ('certain', 2),\n",
       "  ('certainly', 9),\n",
       "  ('cesse', 1),\n",
       "  ('cette', 1),\n",
       "  ('chain', 1),\n",
       "  ('chair', 1),\n",
       "  ('chambre', 1),\n",
       "  ('chance', 8),\n",
       "  ('change', 5),\n",
       "  ('chaos', 2),\n",
       "  ('character', 14),\n",
       "  ('charm', 21),\n",
       "  ('charming', 34),\n",
       "  ('chat', 8),\n",
       "  ('chatting', 1),\n",
       "  ('chaude', 1),\n",
       "  ('cheap', 6),\n",
       "  ('check', 24),\n",
       "  ('cheerful', 1),\n",
       "  ('cheery', 1),\n",
       "  ('chercher', 2),\n",
       "  ('chic', 1),\n",
       "  ('child', 2),\n",
       "  ('chill', 1),\n",
       "  ('chilly', 1),\n",
       "  ('chocolate_chip', 2),\n",
       "  ('choice', 4),\n",
       "  ('choose', 8),\n",
       "  ('cidade', 1),\n",
       "  ('city', 54),\n",
       "  ('clamor', 1),\n",
       "  ('class', 1),\n",
       "  ('clean', 96),\n",
       "  ('clear', 1),\n",
       "  ('clearly', 2),\n",
       "  ('clef', 1),\n",
       "  ('close', 77),\n",
       "  ('closet', 2),\n",
       "  ('cloud', 1),\n",
       "  ('clown', 1),\n",
       "  ('club', 1),\n",
       "  ('clue', 1),\n",
       "  ('cluster', 1),\n",
       "  ('coach', 1),\n",
       "  ('coast', 1),\n",
       "  ('coat', 1),\n",
       "  ('cobbler', 1),\n",
       "  ('code', 1),\n",
       "  ('coffee', 66),\n",
       "  ('coin', 1),\n",
       "  ('colada', 1),\n",
       "  ('cold', 1),\n",
       "  ('colle', 1),\n",
       "  ('collection', 2),\n",
       "  ('colly', 2),\n",
       "  ('color', 7),\n",
       "  ('colored', 1),\n",
       "  ('colorful', 17),\n",
       "  ('colorfull', 1),\n",
       "  ('colour', 1),\n",
       "  ('colourful', 1),\n",
       "  ('com', 1),\n",
       "  ('combine', 2),\n",
       "  ('come', 62),\n",
       "  ('comfie', 1),\n",
       "  ('comfort', 4),\n",
       "  ('comfortable', 173),\n",
       "  ('comfortably', 1),\n",
       "  ('comforting', 1),\n",
       "  ('comfy', 25),\n",
       "  ('commendation', 1),\n",
       "  ('comment', 1),\n",
       "  ('commercial', 1),\n",
       "  ('communal', 1),\n",
       "  ('communicate', 2),\n",
       "  ('communicating', 1),\n",
       "  ('communication', 8),\n",
       "  ('communicative', 3),\n",
       "  ('communicator', 1),\n",
       "  ('community', 3),\n",
       "  ('como', 2),\n",
       "  ('comoda', 1),\n",
       "  ('companion', 1),\n",
       "  ('company', 4),\n",
       "  ('complaint', 1),\n",
       "  ('complement', 1),\n",
       "  ('complete', 1),\n",
       "  ('completely', 3),\n",
       "  ('con', 3),\n",
       "  ('concern', 2),\n",
       "  ('concerned', 1),\n",
       "  ('concert', 3),\n",
       "  ('condition', 1),\n",
       "  ('conection', 1),\n",
       "  ('confirm', 1),\n",
       "  ('conflict', 1),\n",
       "  ('confortable', 1),\n",
       "  ('confortavel', 1),\n",
       "  ('conhecer', 1),\n",
       "  ('conjure', 1),\n",
       "  ('connect', 1),\n",
       "  ('connected', 2),\n",
       "  ('connection', 1),\n",
       "  ('conseil', 2),\n",
       "  ('consider', 6),\n",
       "  ('considerate', 3),\n",
       "  ('constantly', 1),\n",
       "  ('contact', 3),\n",
       "  ('contain', 1),\n",
       "  ('continue', 3),\n",
       "  ('contrast', 1),\n",
       "  ('contribute', 1),\n",
       "  ('control', 1),\n",
       "  ('convenience', 3),\n",
       "  ('convenient', 22),\n",
       "  ('conveniently', 1),\n",
       "  ('conveniently_locate', 2),\n",
       "  ('conveniently_located', 1),\n",
       "  ('conversar', 1),\n",
       "  ('conversation', 30),\n",
       "  ('conversational', 1),\n",
       "  ('cook', 1),\n",
       "  ('cooked', 3),\n",
       "  ('cool', 20),\n",
       "  ('coordinate', 1),\n",
       "  ('corn', 1),\n",
       "  ('corner', 11),\n",
       "  ('correct', 1),\n",
       "  ('correspond', 2),\n",
       "  ('cost', 4),\n",
       "  ('cosy', 5),\n",
       "  ('couch', 2),\n",
       "  ('count', 1),\n",
       "  ('counter', 1),\n",
       "  ('country', 4),\n",
       "  ('couple', 12),\n",
       "  ('course', 4),\n",
       "  ('courteous', 2),\n",
       "  ('courtesy', 1),\n",
       "  ('cousin', 1),\n",
       "  ('coversation', 1),\n",
       "  ('cozinha', 1),\n",
       "  ('cozy', 45),\n",
       "  ('creak', 1),\n",
       "  ('creaky', 1),\n",
       "  ('creamer', 1),\n",
       "  ('create', 2),\n",
       "  ('creative', 1),\n",
       "  ('creatively', 1),\n",
       "  ('crepe', 1),\n",
       "  ('crisis', 1),\n",
       "  ('cross_country', 1),\n",
       "  ('crossed', 1),\n",
       "  ('crucial', 1),\n",
       "  ('cuddle', 2),\n",
       "  ('cuisine', 1),\n",
       "  ('cultural', 2),\n",
       "  ('culture', 3),\n",
       "  ('cup', 4),\n",
       "  ('currently', 1),\n",
       "  ('cut', 8),\n",
       "  ('cute', 37),\n",
       "  ('cycling', 1),\n",
       "  ('da', 1),\n",
       "  ('dan', 3),\n",
       "  ('dancing', 1),\n",
       "  ('dans_un', 2),\n",
       "  ('dark', 1),\n",
       "  ('darkness', 1),\n",
       "  ('darle', 1),\n",
       "  ('darling', 1),\n",
       "  ('darn', 1),\n",
       "  ('das', 1),\n",
       "  ('date', 1),\n",
       "  ('daughter', 18),\n",
       "  ('day', 45),\n",
       "  ('daylight', 1),\n",
       "  ('dead', 1),\n",
       "  ('deal', 2),\n",
       "  ('dear', 1),\n",
       "  ('decide', 3),\n",
       "  ('decision', 1),\n",
       "  ('decor', 7),\n",
       "  ('decorate', 14),\n",
       "  ('decoration', 3),\n",
       "  ('deer', 1),\n",
       "  ('defiently', 1),\n",
       "  ('definately', 1),\n",
       "  ('definetely', 1),\n",
       "  ('definite', 2),\n",
       "  ('definitely', 120),\n",
       "  ('definition', 2),\n",
       "  ('definitley', 1),\n",
       "  ('deixou', 2),\n",
       "  ('dejeuner', 1),\n",
       "  ('delay', 4),\n",
       "  ('delicate', 1),\n",
       "  ('delicioso', 1),\n",
       "  ('delicious', 25),\n",
       "  ('delight', 2),\n",
       "  ('delighted', 2),\n",
       "  ('delightful', 5),\n",
       "  ('dem', 4),\n",
       "  ('demande', 2),\n",
       "  ('demander', 1),\n",
       "  ('den', 2),\n",
       "  ('departure', 2),\n",
       "  ('depois', 1),\n",
       "  ('der', 2),\n",
       "  ('describe', 17),\n",
       "  ('description', 3),\n",
       "  ('deserve', 1),\n",
       "  ('desire', 3),\n",
       "  ('desk', 2),\n",
       "  ('desperate', 1),\n",
       "  ('dessert', 1),\n",
       "  ('destination', 2),\n",
       "  ('detail', 2),\n",
       "  ('deter', 2),\n",
       "  ('determine', 1),\n",
       "  ('detour', 1),\n",
       "  ('deu', 1),\n",
       "  ('devant', 1),\n",
       "  ('dia', 1),\n",
       "  ('dicas', 1),\n",
       "  ('die', 6),\n",
       "  ('different', 7),\n",
       "  ('difficult', 1),\n",
       "  ('dining', 2),\n",
       "  ('dinner', 7),\n",
       "  ('direction', 6),\n",
       "  ('directly', 5),\n",
       "  ('disappoint', 1),\n",
       "  ('disappointed', 2),\n",
       "  ('discover', 2),\n",
       "  ('discussion', 1),\n",
       "  ('discutait', 1),\n",
       "  ('disposicao', 1),\n",
       "  ('distance', 29),\n",
       "  ('distant', 1),\n",
       "  ('district', 2),\n",
       "  ('do', 4),\n",
       "  ('dog', 51),\n",
       "  ('doggie', 1),\n",
       "  ('donne', 2),\n",
       "  ('door', 20),\n",
       "  ('doorstep', 1),\n",
       "  ('dort', 1),\n",
       "  ('double', 1),\n",
       "  ('doubt', 1),\n",
       "  ('downright', 1),\n",
       "  ('downtown', 140),\n",
       "  ('drank', 1),\n",
       "  ('draw', 2),\n",
       "  ('drawback', 1),\n",
       "  ('drawer', 1),\n",
       "  ('dream', 2),\n",
       "  ('drink', 4),\n",
       "  ('drive', 48),\n",
       "  ('driveway', 1),\n",
       "  ('driving', 1),\n",
       "  ('drop', 2),\n",
       "  ('dry', 1),\n",
       "  ('du', 1),\n",
       "  ('du_centre', 1),\n",
       "  ('duda', 1),\n",
       "  ('dudas', 1),\n",
       "  ('due', 4),\n",
       "  ('dug', 1),\n",
       "  ('duration', 1),\n",
       "  ('duty', 1),\n",
       "  ('ear', 2),\n",
       "  ('early', 11),\n",
       "  ('earplug', 1),\n",
       "  ('ease', 3),\n",
       "  ('easiest', 1),\n",
       "  ('easily', 7),\n",
       "  ('easily_accessible', 4),\n",
       "  ('east', 16),\n",
       "  ('eastern', 1),\n",
       "  ('eastside', 1),\n",
       "  ('easy', 59),\n",
       "  ('easygoing', 1),\n",
       "  ('eat', 37),\n",
       "  ('eatery', 2),\n",
       "  ('eau', 1),\n",
       "  ('eclectic', 8),\n",
       "  ('eclipse', 1),\n",
       "  ('ecouter', 1),\n",
       "  ('effectively', 1),\n",
       "  ('effort', 1),\n",
       "  ('egg', 3),\n",
       "  ('ein', 2),\n",
       "  ('eine', 2),\n",
       "  ('einem', 2),\n",
       "  ('eingerichtet', 1),\n",
       "  ('ela', 3),\n",
       "  ('electric', 1),\n",
       "  ('elle', 3),\n",
       "  ('else', 6),\n",
       "  ('email', 2),\n",
       "  ('emergency', 2),\n",
       "  ('empfang', 1),\n",
       "  ('empfehlen', 2),\n",
       "  ('empressa', 1),\n",
       "  ('empty', 1),\n",
       "  ('en', 1),\n",
       "  ('encantador', 1),\n",
       "  ('encantadore', 1),\n",
       "  ('enchant', 1),\n",
       "  ('enchantingly', 1),\n",
       "  ('encounter', 4),\n",
       "  ('end', 8),\n",
       "  ('endear', 1),\n",
       "  ('endless', 2),\n",
       "  ('energy', 4),\n",
       "  ('engage', 4),\n",
       "  ('engagement', 1),\n",
       "  ('enjoy', 80),\n",
       "  ('enjoyable', 8),\n",
       "  ('enormous', 1),\n",
       "  ('enough', 42),\n",
       "  ('ensure', 4),\n",
       "  ('enter', 2),\n",
       "  ('entertainment', 3),\n",
       "  ('entire', 3),\n",
       "  ('entrance', 1),\n",
       "  ('entspannen', 1),\n",
       "  ('envelop', 1),\n",
       "  ('environment', 2),\n",
       "  ('epitome', 1),\n",
       "  ('equally', 1),\n",
       "  ('equipped', 2),\n",
       "  ('erg', 1),\n",
       "  ('erreichten', 1),\n",
       "  ('es', 1),\n",
       "  ('eso', 1),\n",
       "  ('especially', 15),\n",
       "  ('essential', 1),\n",
       "  ('esta', 1),\n",
       "  ('et', 9),\n",
       "  ('etait', 1),\n",
       "  ('etant', 1),\n",
       "  ('ete', 1),\n",
       "  ('eu', 1),\n",
       "  ('euhig', 1),\n",
       "  ('even', 51),\n",
       "  ('evening', 10),\n",
       "  ('event', 3),\n",
       "  ('ever', 34),\n",
       "  ('everyday', 2),\n",
       "  ('everywhere', 5),\n",
       "  ('evident', 1),\n",
       "  ('evtl', 1),\n",
       "  ('exact', 1),\n",
       "  ('exactly', 11),\n",
       "  ('example', 1),\n",
       "  ('exceed', 1),\n",
       "  ('exceedingly', 1),\n",
       "  ('excellent', 35),\n",
       "  ('excepted', 1),\n",
       "  ('exceptional', 2),\n",
       "  ('exceptionally', 1),\n",
       "  ('excite', 1),\n",
       "  ('exciting', 1),\n",
       "  ('excuse', 1),\n",
       "  ('exemplary', 1),\n",
       "  ('exercise', 2),\n",
       "  ('exotic', 1),\n",
       "  ('expect', 15),\n",
       "  ('expectation', 3),\n",
       "  ('experiance', 1),\n",
       "  ('experience', 102),\n",
       "  ('experienced', 1),\n",
       "  ('explain', 2),\n",
       "  ('explore', 18),\n",
       "  ('expressway', 1),\n",
       "  ('extend', 4),\n",
       "  ('extended', 1),\n",
       "  ('extensively', 1),\n",
       "  ('extent', 1),\n",
       "  ('exterior', 1),\n",
       "  ('extra', 14),\n",
       "  ('extraordinaire', 1),\n",
       "  ('extraordinarily', 1),\n",
       "  ('extraordinary', 2),\n",
       "  ('extremely', 28),\n",
       "  ('eye', 3),\n",
       "  ('eye_mask', 1),\n",
       "  ('fab', 1),\n",
       "  ('fabulous', 13),\n",
       "  ('face', 1),\n",
       "  ('facemask', 1),\n",
       "  ('facility', 3),\n",
       "  ('fact', 5),\n",
       "  ('fairly', 2),\n",
       "  ('fait', 5),\n",
       "  ('fall', 4),\n",
       "  ('falling_asleep', 1),\n",
       "  ('familial', 1),\n",
       "  ('familiar', 1),\n",
       "  ('familie', 1),\n",
       "  ('family', 145),\n",
       "  ('famous', 1),\n",
       "  ('fan', 1),\n",
       "  ('fanatic', 1),\n",
       "  ('fantastic', 29),\n",
       "  ('fantastically', 1),\n",
       "  ('far', 20),\n",
       "  ('fareast', 1),\n",
       "  ('fast', 2),\n",
       "  ('favor', 1),\n",
       "  ('favorite', 8),\n",
       "  ('favourite', 4),\n",
       "  ('fear', 1),\n",
       "  ('feel', 217),\n",
       "  ('feeling', 5),\n",
       "  ('fell', 1),\n",
       "  ('female', 1),\n",
       "  ('ferme', 1),\n",
       "  ('festival', 1),\n",
       "  ('fez', 1),\n",
       "  ('ficou', 1),\n",
       "  ('fig', 1),\n",
       "  ('figuratively', 1),\n",
       "  ('figure', 1),\n",
       "  ('fill', 7),\n",
       "  ('fille', 3),\n",
       "  ('filling', 1),\n",
       "  ('filtered_water', 1),\n",
       "  ('final', 2),\n",
       "  ('find', 27),\n",
       "  ('finding', 1),\n",
       "  ('fine', 4),\n",
       "  ('finest', 1),\n",
       "  ('fire', 1),\n",
       "  ('firefly', 1),\n",
       "  ('first', 67),\n",
       "  ('fit', 1),\n",
       "  ('five_point', 7),\n",
       "  ('fix', 1),\n",
       "  ('fixing', 1),\n",
       "  ('flask', 1),\n",
       "  ('fleece', 1),\n",
       "  ('flexible', 10),\n",
       "  ('flight', 5),\n",
       "  ('floor', 1),\n",
       "  ('floorboard', 1),\n",
       "  ('fluffy', 1),\n",
       "  ('fly', 1),\n",
       "  ('folk', 5),\n",
       "  ('follow', 3),\n",
       "  ('fond', 1),\n",
       "  ('food', 16),\n",
       "  ('foot', 1),\n",
       "  ('forever', 4),\n",
       "  ('forget', 2),\n",
       "  ('form', 1),\n",
       "  ('former', 1),\n",
       "  ('fortunate', 1),\n",
       "  ('forward', 2),\n",
       "  ('fragen', 1),\n",
       "  ('free', 3),\n",
       "  ('freedom', 1),\n",
       "  ('freeway', 1),\n",
       "  ('french', 1),\n",
       "  ('fresh', 18),\n",
       "  ('fresh_flower', 2),\n",
       "  ('fresh_fruit', 1),\n",
       "  ('freshly', 1),\n",
       "  ('freundliche', 1),\n",
       "  ('fridge', 10),\n",
       "  ('friend', 50),\n",
       "  ('friendliness', 3),\n",
       "  ('friendly', 94),\n",
       "  ('frig', 1),\n",
       "  ('frigo', 1),\n",
       "  ('front', 7),\n",
       "  ('front_porch', 3),\n",
       "  ('fruit', 6),\n",
       "  ('fuera', 1),\n",
       "  ('full', 23),\n",
       "  ('fully', 1),\n",
       "  ('fun', 19),\n",
       "  ('funky', 4),\n",
       "  ('funny', 2),\n",
       "  ('fur', 3),\n",
       "  ('furnished', 4),\n",
       "  ('furniture', 1),\n",
       "  ('furry', 3),\n",
       "  ('furthermore', 1),\n",
       "  ('fut', 1),\n",
       "  ('futon', 9),\n",
       "  ('future', 5),\n",
       "  ('gain', 1),\n",
       "  ('gal', 1),\n",
       "  ('game', 1),\n",
       "  ('gang', 1),\n",
       "  ('garden', 4),\n",
       "  ('gateau', 1),\n",
       "  ('gaylord', 1),\n",
       "  ('gear', 1),\n",
       "  ('geeignet', 1),\n",
       "  ('gehad', 1),\n",
       "  ('gelegen', 1),\n",
       "  ('gem', 2),\n",
       "  ('gen', 1),\n",
       "  ('general', 1),\n",
       "  ('generally', 1),\n",
       "  ('genererous', 1),\n",
       "  ('generosity', 4),\n",
       "  ('generous', 18),\n",
       "  ('generously', 2),\n",
       "  ('genommen', 1),\n",
       "  ('genoten', 1),\n",
       "  ('gentillesse', 1),\n",
       "  ('gentleman', 1),\n",
       "  ('genuine', 6),\n",
       "  ('genuinely', 2),\n",
       "  ('gepflegt', 1),\n",
       "  ('gerne', 1),\n",
       "  ('get', 99),\n",
       "  ('getaway', 1),\n",
       "  ('gewesen', 1),\n",
       "  ('giant', 1),\n",
       "  ('gibt', 1),\n",
       "  ('gift', 3),\n",
       "  ('gigantic', 1),\n",
       "  ('girl', 1),\n",
       "  ('girlfriend', 6),\n",
       "  ('give', 84),\n",
       "  ('glad', 4),\n",
       "  ('gladly', 1),\n",
       "  ('glass', 2),\n",
       "  ('glove', 1),\n",
       "  ('go', 113),\n",
       "  ('golf_course', 2),\n",
       "  ('good', 104),\n",
       "  ('goodbye', 1),\n",
       "  ('gorgeous', 17),\n",
       "  ('gracious', 13),\n",
       "  ('graciously', 3),\n",
       "  ('graciousness', 1),\n",
       "  ('grain', 1),\n",
       "  ('grande', 1),\n",
       "  ('grateful', 5),\n",
       "  ('gratefull', 1),\n",
       "  ('gratious', 1),\n",
       "  ('great', 2),\n",
       "  ('greatly', 1),\n",
       "  ('green', 5),\n",
       "  ('greenbelt', 1),\n",
       "  ('greenway', 1),\n",
       "  ('greet', 18),\n",
       "  ('greeting', 1),\n",
       "  ('gross', 1),\n",
       "  ('grossen', 1),\n",
       "  ('ground', 1),\n",
       "  ('group', 1),\n",
       "  ('grow', 1),\n",
       "  ('grunen', 1),\n",
       "  ('guess', 1),\n",
       "  ('guest', 21),\n",
       "  ('guidance', 1),\n",
       "  ('guide', 1),\n",
       "  ('gush', 1),\n",
       "  ('gut', 2),\n",
       "  ('guy', 6),\n",
       "  ('haar', 1),\n",
       "  ('habitacion', 1),\n",
       "  ('hailstorm', 1),\n",
       "  ('hair', 1),\n",
       "  ('half', 1),\n",
       "  ('hammock', 4),\n",
       "  ('han', 1),\n",
       "  ('hand', 2),\n",
       "  ('handle', 1),\n",
       "  ('handy', 1),\n",
       "  ('hang', 4),\n",
       "  ('hangout', 1),\n",
       "  ('happen', 1),\n",
       "  ('happening', 1),\n",
       "  ('happily', 1),\n",
       "  ('happiness', 2),\n",
       "  ('happy', 14),\n",
       "  ('hard', 4),\n",
       "  ('hare', 1),\n",
       "  ('harzlich', 1),\n",
       "  ('hassle', 1),\n",
       "  ('hat', 2),\n",
       "  ('hatten', 3),\n",
       "  ('haunt', 1),\n",
       "  ('head', 7),\n",
       "  ('headache', 1),\n",
       "  ('hear', 4),\n",
       "  ('hearing', 1),\n",
       "  ('heart', 10),\n",
       "  ('heartbeat', 3),\n",
       "  ('hearted', 3),\n",
       "  ('heartfelt', 1),\n",
       "  ('heartwarmed', 1),\n",
       "  ('heartwarming', 1),\n",
       "  ('heat', 1),\n",
       "  ('heating', 1),\n",
       "  ('heave', 1),\n",
       "  ('hebben', 2),\n",
       "  ('hecho', 1),\n",
       "  ('heerlijk', 1),\n",
       "  ('help', 24),\n",
       "  ('helpful', 63),\n",
       "  ('helpfulness', 1),\n",
       "  ('helping', 1),\n",
       "  ('hesitate', 6),\n",
       "  ('hesitation', 2),\n",
       "  ('hidden', 1),\n",
       "  ('hide', 5),\n",
       "  ('high', 3),\n",
       "  ('highlight', 6),\n",
       "  ('highly', 54),\n",
       "  ('hint', 4),\n",
       "  ('hinter', 1),\n",
       "  ('hip', 8),\n",
       "  ('historic', 5),\n",
       "  ('historical', 1),\n",
       "  ('history', 3),\n",
       "  ('hit', 1),\n",
       "  ('hockey', 1),\n",
       "  ('hold', 1),\n",
       "  ('holiday', 1),\n",
       "  ('home', 1),\n",
       "  ('homebase', 2),\n",
       "  ('homecooke', 1),\n",
       "  ('homelike', 1),\n",
       "  ('homelikeairbnb', 1),\n",
       "  ('homely', 5),\n",
       "  ('homemade', 9),\n",
       "  ('homemade_scone', 1),\n",
       "  ('homey', 8),\n",
       "  ('homley', 1),\n",
       "  ('honestly', 2),\n",
       "  ('honey', 2),\n",
       "  ('hop', 1),\n",
       "  ('hope', 24),\n",
       "  ('hopefully', 1),\n",
       "  ('hoping', 1),\n",
       "  ('hospitable', 26),\n",
       "  ('hospital', 1),\n",
       "  ('hospitality', 36),\n",
       "  ('host', 169),\n",
       "  ('hostess', 7),\n",
       "  ('hot', 7),\n",
       "  ('hotel', 5),\n",
       "  ('hour', 9),\n",
       "  ('house', 4),\n",
       "  ('houseflow', 1),\n",
       "  ('household', 5),\n",
       "  ('houselet', 1),\n",
       "  ('housemate', 2),\n",
       "  ('hub', 1),\n",
       "  ('hug', 7),\n",
       "  ('huge', 9),\n",
       "  ('hugged', 1),\n",
       "  ('human', 1),\n",
       "  ('humanity', 1),\n",
       "  ('humor', 1),\n",
       "  ('hunt', 1),\n",
       "  ('hunting', 1),\n",
       "  ('husband', 109),\n",
       "  ('hydrant', 1),\n",
       "  ('hypothesis', 1),\n",
       "  ('ice', 1),\n",
       "  ('ice_cream', 1),\n",
       "  ('idea', 1),\n",
       "  ('ideal', 6),\n",
       "  ('ihre', 1),\n",
       "  ('ihrer', 1),\n",
       "  ('ill', 2),\n",
       "  ('imagine', 5),\n",
       "  ('immaculate', 4),\n",
       "  ('immediately', 7),\n",
       "  ('impeccably', 1),\n",
       "  ('importantly', 1),\n",
       "  ('impressed', 3),\n",
       "  ('impression', 1),\n",
       "  ('in', 1),\n",
       "  ('inane', 1),\n",
       "  ('inch', 1),\n",
       "  ('include', 18),\n",
       "  ('incluso', 1),\n",
       "  ('incredible', 15),\n",
       "  ('incredibly', 25),\n",
       "  ('increible', 1),\n",
       "  ('indeed', 2),\n",
       "  ('independent', 1),\n",
       "  ('indicative', 1),\n",
       "  ('indie', 1),\n",
       "  ('indiqua', 1),\n",
       "  ('indiquee', 1),\n",
       "  ('individual', 2),\n",
       "  ('indoor', 1),\n",
       "  ('inexpensive', 1),\n",
       "  ('info', 1),\n",
       "  ('inform', 1),\n",
       "  ('information', 13),\n",
       "  ('informativ', 1),\n",
       "  ('informative', 2),\n",
       "  ('informiert', 1),\n",
       "  ('inhabit', 1),\n",
       "  ('inhabitant', 1),\n",
       "  ('inhabitee', 1),\n",
       "  ('innert', 1),\n",
       "  ('inside', 3),\n",
       "  ('inspiration', 1),\n",
       "  ('inspire', 1),\n",
       "  ('instant', 1),\n",
       "  ('instantly', 2),\n",
       "  ('instead', 3),\n",
       "  ('instrument', 1),\n",
       "  ('integrate', 1),\n",
       "  ('intelligent', 3),\n",
       "  ('intend', 1),\n",
       "  ...]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,        # Replace with gensim.models.ldamodel.LdaModel()\n",
    "                       id2word=id2word,\n",
    "                       num_topics=8, #number of topics to identify\n",
    "                       random_state=100,\n",
    "                       #update_every=1,                          #Add back in with LdaModel\n",
    "                       chunksize=100, #number of documents to pass per chunk\n",
    "                       passes=10, #number of training passes\n",
    "                       #alpha='auto',                            #Add back in with LdaModel\n",
    "                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.018*\"room\" + 0.017*\"clean\" + 0.016*\"hostel\" + 0.011*\"staff\" + '\n",
      "  '0.010*\"good\" + 0.010*\"downtown\" + 0.009*\"friendly\" + 0.009*\"stay\" + '\n",
      "  '0.009*\"recommend\" + 0.009*\"make\"'),\n",
      " (1,\n",
      "  '0.010*\"beautiful\" + 0.008*\"comfortable\" + 0.008*\"art\" + 0.008*\"make\" + '\n",
      "  '0.007*\"wonderful\" + 0.006*\"well\" + 0.006*\"feel\" + 0.006*\"neighborhood\" + '\n",
      "  '0.006*\"perfect\" + 0.006*\"recommend\"'),\n",
      " (2,\n",
      "  '0.026*\"apartment\" + 0.014*\"bar\" + 0.012*\"clean\" + 0.011*\"walk\" + '\n",
      "  '0.011*\"restaurant\" + 0.008*\"night\" + 0.008*\"perfect\" + 0.008*\"get\" + '\n",
      "  '0.008*\"definitely\" + 0.008*\"recommend\"'),\n",
      " (3,\n",
      "  '0.012*\"perfect\" + 0.012*\"downtown\" + 0.012*\"recommend\" + 0.011*\"clean\" + '\n",
      "  '0.011*\"group\" + 0.010*\"time\" + 0.010*\"space\" + 0.010*\"well\" + 0.010*\"need\" '\n",
      "  '+ 0.009*\"comfortable\"'),\n",
      " (4,\n",
      "  '0.013*\"room\" + 0.012*\"comfortable\" + 0.012*\"make\" + 0.011*\"recommend\" + '\n",
      "  '0.011*\"stay\" + 0.010*\"feel\" + 0.010*\"time\" + 0.010*\"host\" + 0.009*\"clean\" + '\n",
      "  '0.009*\"cottage\"'),\n",
      " (5,\n",
      "  '0.021*\"clean\" + 0.018*\"downtown\" + 0.014*\"comfortable\" + 0.012*\"recommend\" '\n",
      "  '+ 0.011*\"definitely\" + 0.011*\"need\" + 0.011*\"close\" + 0.011*\"nice\" + '\n",
      "  '0.010*\"easy\" + 0.010*\"perfect\"'),\n",
      " (6,\n",
      "  '0.020*\"room\" + 0.017*\"clean\" + 0.014*\"comfortable\" + 0.014*\"nice\" + '\n",
      "  '0.011*\"quiet\" + 0.011*\"recommend\" + 0.010*\"make\" + 0.009*\"need\" + '\n",
      "  '0.009*\"coffee\" + 0.008*\"stay\"'),\n",
      " (7,\n",
      "  '0.015*\"clean\" + 0.013*\"walk\" + 0.011*\"recommend\" + 0.011*\"perfect\" + '\n",
      "  '0.011*\"downtown\" + 0.010*\"comfortable\" + 0.010*\"restaurant\" + '\n",
      "  '0.010*\"apartment\" + 0.010*\"neighborhood\" + 0.010*\"need\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 Keywords in each grouped Topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.65874981679059\n",
      "\n",
      "Coherence Score:  0.2618261734711568\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score - Likely more helpful. Takes a while to run.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual for Viewing each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping step 17 about finding the best number of topics - Tim recommends 8-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find dominant Topic in each Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-57075f752869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "\n",
    "for pdf, doc in tqdm(zip(pdfs, docs)):\n",
    "    bow = corpora.Dictionary(data_lemmatized)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    for res in topics:\n",
    "        topic, pct = res\n",
    "        contents.append({'pdf': pdf, 'topic': topic, 'percent': pct})\n",
    "\n",
    "topics = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-a911974a7b3d>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get main topic in each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_topic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most representative Reviews for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "\n",
    "        file = topics[topics.topic == i].sort_values('percent', ascending = False).head().iloc[j, 0]\n",
    "\n",
    "        cmd = f'cp {file} representative_docs/topic_{i + 1}/.'\n",
    "\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Housing Project (this isn't working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a previous code, for safekeeping.\n",
    "# Create a tokenizing function that takes text and removes all numbers\n",
    "#def tokenizer(text):\n",
    "#    return [x for x in re.findall(r'[a-z]+', text.lower()) if len(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a list comprehension, iterate over the four permit type contents and vectorize them using the TfidfVectorizer\n",
    "vectorizer_test1 = TfidfVectorizer(\n",
    "    tokenizer = tokenizer, \n",
    "    stop_words = 'english', \n",
    "    #min_df=50, \n",
    "    #max_df=0.4, \n",
    "    #ngram_range=(1,3)\n",
    ").fit(airbnb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see the shape of the first sparse matrix generated\n",
    "vectorizer_test1.transform(airbnb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Gensim Blog\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = airbnb.comments_concatenated.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original document: ')\n",
    "words = []\n",
    "for word in test_comment.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(test_comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again from Michael's Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = airbnb.copy()['comments_concatenated'].iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in tqdm(range(len(docs))):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 2] for doc in docs]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest of Michael's walkthrough below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 6\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
